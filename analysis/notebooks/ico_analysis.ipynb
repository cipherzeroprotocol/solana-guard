{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a112b2",
   "metadata": {},
   "source": [
    "# Initial Coin Offering (ICO) Analysis Notebook\n",
    "\n",
    "This notebook analyzes Solana ICOs and token launches for potential risks, scams, and anomalies using SolanaGuard's analytical tools.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Initial Coin Offerings (ICOs) and token launches on Solana present both opportunities and risks. This analysis aims to identify:\n",
    "\n",
    "1. **Token distribution patterns** - How widely distributed is the token at launch?\n",
    "2. **Team wallet behavior** - Are the project founders/team acting responsibly?\n",
    "3. **Launch metrics** - Analysis of liquidity, pricing, and market activity\n",
    "4. **Post-launch activity** - Monitoring for suspicious behavior after token launch\n",
    "5. **Risk indicators** - Identifying common patterns in fraudulent ICOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b03cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import project config\n",
    "import config\n",
    "\n",
    "# Import SolanaGuard data collectors\n",
    "from data_collection.collectors.helius_collector import HeliusCollector\n",
    "from data_collection.collectors.rugcheck_collector import RugCheckCollector\n",
    "from data_collection.collectors.vybe_collector import VybeCollector\n",
    "\n",
    "# Configure plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(config.REPORTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157974f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for ICO analysis\n",
    "\n",
    "def calculate_gini_coefficient(values):\n",
    "    \"\"\"\n",
    "    Calculate Gini coefficient (measure of inequality)\n",
    "    0 = perfect equality, 1 = perfect inequality\n",
    "    \n",
    "    Args:\n",
    "        values: Array of values (e.g., token amounts)\n",
    "        \n",
    "    Returns:\n",
    "        Gini coefficient (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Sort values in ascending order\n",
    "    values = np.asarray(values)\n",
    "    values = values[np.isfinite(values)]\n",
    "    if len(values) <= 1 or np.sum(values) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Sort and cumulate values\n",
    "    values = np.sort(values)\n",
    "    index = np.arange(1, len(values) + 1)\n",
    "    n = len(values)\n",
    "    \n",
    "    # Calculate Gini coefficient\n",
    "    return ((2 * np.sum(np.multiply(index, values))) / (n * np.sum(values))) - ((n + 1) / n)\n",
    "\n",
    "\n",
    "def calculate_ico_metrics(price_history, liquidity_data):\n",
    "    \"\"\"\n",
    "    Calculate key metrics for token launches based on price and liquidity history\n",
    "    \n",
    "    Args:\n",
    "        price_history: List of price data points (OHLCV)\n",
    "        liquidity_data: List of liquidity data points\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of ICO metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Price metrics\n",
    "    if price_history:\n",
    "        # Extract price data\n",
    "        try:\n",
    "            # Initial price (first data point)\n",
    "            metrics[\"initial_price\"] = price_history[0][\"close\"] if \"close\" in price_history[0] else 0\n",
    "            \n",
    "            # Peak price\n",
    "            prices = [p[\"close\"] for p in price_history if \"close\" in p]\n",
    "            metrics[\"price_peak\"] = max(prices) if prices else 0\n",
    "            \n",
    "            # 7-day price (last data point)\n",
    "            metrics[\"price_7d\"] = price_history[-1][\"close\"] if \"close\" in price_history[-1] else 0\n",
    "            \n",
    "            # Price change percentage\n",
    "            if metrics[\"initial_price\"] > 0:\n",
    "                metrics[\"price_change_pct\"] = ((metrics[\"price_7d\"] - metrics[\"initial_price\"]) / metrics[\"initial_price\"]) * 100\n",
    "            else:\n",
    "                metrics[\"price_change_pct\"] = 0\n",
    "            \n",
    "            # Price volatility\n",
    "            if len(prices) > 1:\n",
    "                price_changes = [prices[i+1]/prices[i] - 1 for i in range(len(prices)-1) if prices[i] > 0]\n",
    "                metrics[\"price_volatility\"] = np.std(price_changes) * 100 if price_changes else 0\n",
    "            else:\n",
    "                metrics[\"price_volatility\"] = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating price metrics: {e}\")\n",
    "    \n",
    "    # Liquidity metrics\n",
    "    if liquidity_data:\n",
    "        try:\n",
    "            # Initial liquidity\n",
    "            metrics[\"initial_liquidity\"] = liquidity_data[0][\"liquidity_usd\"] if isinstance(liquidity_data[0], dict) and \"liquidity_usd\" in liquidity_data[0] else 0\n",
    "            \n",
    "            # Final liquidity\n",
    "            metrics[\"final_liquidity\"] = liquidity_data[-1][\"liquidity_usd\"] if isinstance(liquidity_data[-1], dict) and \"liquidity_usd\" in liquidity_data[-1] else 0\n",
    "            \n",
    "            # Liquidity change percentage\n",
    "            if metrics[\"initial_liquidity\"] > 0:\n",
    "                metrics[\"liquidity_change_pct\"] = ((metrics[\"final_liquidity\"] - metrics[\"initial_liquidity\"]) / metrics[\"initial_liquidity\"]) * 100\n",
    "            else:\n",
    "                metrics[\"liquidity_change_pct\"] = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating liquidity metrics: {e}\")\n",
    "    else:\n",
    "        # If no liquidity data provided, use reasonable defaults\n",
    "        metrics[\"initial_liquidity\"] = 0\n",
    "        metrics[\"final_liquidity\"] = 0\n",
    "        metrics[\"liquidity_change_pct\"] = 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f34bc",
   "metadata": {},
   "source": [
    "## Initialize API Collectors\n",
    "\n",
    "First, we initialize the necessary API collectors to gather data from various sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844fdd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:26,588 - helius_collector - INFO - Initialized Helius collector\n",
      "2025-04-25 00:45:26,594 - vybe_collector - INFO - Initialized Vybe collector\n",
      "2025-04-25 00:45:26,595 - rugcheck_collector - INFO - Initialized RugCheck collector\n",
      "2025-04-25 00:45:26,594 - vybe_collector - INFO - Initialized Vybe collector\n",
      "2025-04-25 00:45:26,595 - rugcheck_collector - INFO - Initialized RugCheck collector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error connecting to APIs: 'HeliusCollector' object has no attribute 'check_connection'\n",
      "Continuing with analysis, but some features may not work properly\n"
     ]
    }
   ],
   "source": [
    "# Initialize collectors\n",
    "helius = HeliusCollector()\n",
    "vybe = VybeCollector()\n",
    "rugcheck = RugCheckCollector()\n",
    "\n",
    "# Verify API connectivity\n",
    "try:\n",
    "    helius_status = helius.check_connection()\n",
    "    rugcheck_status = rugcheck.check_connection()\n",
    "    vybe_status = vybe.check_connection()\n",
    "    print(f\"Helius API: {'Connected' if helius_status else 'Connection failed'}\")\n",
    "    print(f\"RugCheck API: {'Connected' if rugcheck_status else 'Connection failed'}\")\n",
    "    print(f\"Vybe API: {'Connected' if vybe_status else 'Connection failed'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to APIs: {e}\")\n",
    "    print(\"Continuing with analysis, but some features may not work properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efbec5",
   "metadata": {},
   "source": [
    "## Identify Recent ICOs and Token Launches\n",
    "\n",
    "Let's identify recent token launches on Solana for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57870250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for token launches in the last 30 days...\n",
      "Error fetching token launches: 'RugCheckCollector' object has no attribute 'get_recent_token_launches'\n",
      "No recent token launches found, using predefined examples for testing\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch recent token launches\n",
    "def get_recent_token_launches(days_ago=30, min_liquidity=1000, max_results=10):\n",
    "    # Define time range\n",
    "    end_time = int(datetime.now().timestamp())\n",
    "    start_time = int((datetime.now() - timedelta(days=days_ago)).timestamp())\n",
    "    \n",
    "    print(f\"Searching for token launches in the last {days_ago} days...\")\n",
    "    \n",
    "    # Get token launches from RugCheck\n",
    "    try:\n",
    "        launches = rugcheck.get_recent_token_launches(\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            min_liquidity_usd=min_liquidity\n",
    "        )\n",
    "        print(f\"Found {len(launches)} token launches with min liquidity ${min_liquidity}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token launches: {e}\")\n",
    "        launches = []\n",
    "    \n",
    "    # Filter and sort by launch date (most recent first)\n",
    "    if launches:\n",
    "        launches = sorted(launches, key=lambda x: x.get('launch_timestamp', 0), reverse=True)\n",
    "        if max_results and max_results < len(launches):\n",
    "            launches = launches[:max_results]\n",
    "    \n",
    "    return launches\n",
    "\n",
    "# Get recent launches\n",
    "recent_launches = get_recent_token_launches(days_ago=30, min_liquidity=5000)\n",
    "\n",
    "# Display recent launches in a table\n",
    "if recent_launches:\n",
    "    launch_df = pd.DataFrame(recent_launches)\n",
    "    \n",
    "    # Format the display\n",
    "    display_cols = [\n",
    "        'token_symbol', 'token_name', 'mint_address', \n",
    "        'initial_liquidity_usd', 'initial_mcap_usd', 'launch_date'\n",
    "    ]\n",
    "    \n",
    "    # Convert timestamp to date if needed\n",
    "    if 'launch_timestamp' in launch_df.columns and 'launch_date' not in launch_df.columns:\n",
    "        launch_df['launch_date'] = pd.to_datetime(launch_df['launch_timestamp'], unit='s')\n",
    "    \n",
    "    # Select columns that exist\n",
    "    available_cols = [col for col in display_cols if col in launch_df.columns]\n",
    "    display(launch_df[available_cols].head(10))\n",
    "    \n",
    "    # Select tokens for detailed analysis\n",
    "    selected_tokens = launch_df[['token_symbol', 'mint_address']].head(5).values.tolist()\n",
    "    print(f\"\\nSelected {len(selected_tokens)} tokens for detailed analysis\")\n",
    "    for symbol, address in selected_tokens:\n",
    "        print(f\"- {symbol}: {address}\")\n",
    "else:\n",
    "    print(\"No recent token launches found, using predefined examples for testing\")\n",
    "    # Use real token examples for testing\n",
    "    selected_tokens = [\n",
    "        [\"PYTH\", \"HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\"],  # Pyth token\n",
    "        [\"JTO\", \"jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\"],     # Jito token\n",
    "        [\"CROWN\", \"FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\"]  # Crown token\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2a7df",
   "metadata": {},
   "source": [
    "## Analyze Token Distribution and Initial Allocation\n",
    "\n",
    "Let's analyze how the tokens were initially distributed, which is crucial for identifying potential rug pulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1405984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:26,646 - vybe_collector - INFO - Getting top holders for HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing token distribution for PYTH (HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:37,746 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A7B90>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:37,746 - vybe_collector - INFO - Getting top holders for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n",
      "2025-04-25 00:45:37,746 - vybe_collector - INFO - Getting top holders for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n",
      "2025-04-25 00:45:37,790 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A5B50>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:37,792 - vybe_collector - INFO - Getting top holders for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n",
      "2025-04-25 00:45:37,790 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A5B50>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:37,792 - vybe_collector - INFO - Getting top holders for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting token holders: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A7B90>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "\n",
      "Analyzing token distribution for JTO (jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY)\n",
      "Error getting token holders: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A5B50>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "\n",
      "Analyzing token distribution for CROWN (FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:38,082 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A6840>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting token holders: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/top-holders?limit=100&page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A6840>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze token distribution\n",
    "def analyze_token_distribution(mint_address, token_symbol):\n",
    "    print(f\"\\nAnalyzing token distribution for {token_symbol} ({mint_address})\")\n",
    "    \n",
    "    # Get token holders\n",
    "    try:\n",
    "        holders_data = vybe.get_token_top_holders(mint_address)\n",
    "        holders = holders_data.get(\"data\", [])\n",
    "        print(f\"Found {len(holders)} token holders\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token holders: {e}\")\n",
    "        holders = []\n",
    "        \n",
    "    if not holders:\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    holders_df = pd.DataFrame(holders)\n",
    "    \n",
    "    # Calculate distribution metrics\n",
    "    total_supply = holders_df[\"amount\"].sum() if \"amount\" in holders_df.columns else 0\n",
    "    \n",
    "    # Calculate concentration metrics\n",
    "    if \"amount\" in holders_df.columns and total_supply > 0:\n",
    "        holders_df[\"percentage\"] = holders_df[\"amount\"] / total_supply * 100\n",
    "        \n",
    "        # Top holder percentages\n",
    "        top1_pct = holders_df[\"percentage\"].iloc[0] if len(holders_df) >= 1 else 0\n",
    "        top5_pct = holders_df[\"percentage\"].iloc[:5].sum() if len(holders_df) >= 5 else holders_df[\"percentage\"].sum()\n",
    "        top10_pct = holders_df[\"percentage\"].iloc[:10].sum() if len(holders_df) >= 10 else holders_df[\"percentage\"].sum()\n",
    "        \n",
    "        print(f\"Top holder owns {top1_pct:.2f}% of supply\")\n",
    "        print(f\"Top 5 holders own {top5_pct:.2f}% of supply\")\n",
    "        print(f\"Top 10 holders own {top10_pct:.2f}% of supply\")\n",
    "        \n",
    "        # Calculate Gini coefficient (measure of inequality, 0 = perfect equality, 1 = perfect inequality)\n",
    "        if len(holders_df) > 5:\n",
    "            gini = calculate_gini_coefficient(holders_df[\"amount\"].values)\n",
    "            print(f\"Gini coefficient: {gini:.4f}\")\n",
    "        else:\n",
    "            gini = None\n",
    "        \n",
    "        # Identify potential team wallets\n",
    "        team_wallets = []\n",
    "        for index, row in holders_df.head(10).iterrows():\n",
    "            address = row.get(\"address\")\n",
    "            if address:\n",
    "                # Check if this is a team wallet using on-chain indicators\n",
    "                try:\n",
    "                    is_team = rugcheck.check_if_team_wallet(address, mint_address)\n",
    "                    if is_team:\n",
    "                        team_wallets.append({\n",
    "                            \"address\": address,\n",
    "                            \"percentage\": row.get(\"percentage\", 0),\n",
    "                            \"confidence\": is_team.get(\"confidence\", 0)\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        \n",
    "        if team_wallets:\n",
    "            team_pct = sum(w[\"percentage\"] for w in team_wallets)\n",
    "            print(f\"Identified {len(team_wallets)} potential team wallets holding {team_pct:.2f}% of supply\")\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.pie(holders_df[\"percentage\"].head(10), labels=holders_df[\"address\"].head(10).apply(lambda x: x[:6] + \"...\"),\n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "        plt.title(f\"{token_symbol} Token Distribution (Top 10 Holders)\")\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            \"token_symbol\": token_symbol,\n",
    "            \"mint_address\": mint_address,\n",
    "            \"holders_count\": len(holders_df),\n",
    "            \"top1_pct\": top1_pct,\n",
    "            \"top5_pct\": top5_pct,\n",
    "            \"top10_pct\": top10_pct,\n",
    "            \"gini_coefficient\": gini,\n",
    "            \"team_wallets\": team_wallets,\n",
    "            \"team_wallets_pct\": sum(w[\"percentage\"] for w in team_wallets) if team_wallets else 0\n",
    "        }\n",
    "    else:\n",
    "        print(\"Insufficient data for distribution analysis\")\n",
    "        return None\n",
    "\n",
    "# Analyze distribution for each selected token\n",
    "distribution_results = {}\n",
    "for token_symbol, mint_address in selected_tokens:\n",
    "    distribution_results[mint_address] = analyze_token_distribution(mint_address, token_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd89eee",
   "metadata": {},
   "source": [
    "## Analyze Launch Metrics and Price Action\n",
    "\n",
    "Let's examine the launch metrics and initial price action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd9ecb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:38,103 - vybe_collector - INFO - Getting token details for HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing launch metrics for PYTH (HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:38,417 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A4320>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:38,417 - rugcheck_collector - INFO - Getting token report for HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\n",
      "2025-04-25 00:45:38,417 - rugcheck_collector - INFO - Getting token report for HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting token details: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A4320>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:38,673 - rugcheck_collector - ERROR - Failed to make API request: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/report\n",
      "2025-04-25 00:45:38,675 - vybe_collector - INFO - Getting price OHLCV for HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\n",
      "2025-04-25 00:45:38,675 - vybe_collector - INFO - Getting price OHLCV for HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB\n",
      "2025-04-25 00:45:38,751 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926338&timeEnd=1745531138 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A74D0>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:38,751 - vybe_collector - INFO - Getting token details for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n",
      "2025-04-25 00:45:38,751 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926338&timeEnd=1745531138 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A74D0>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:38,751 - vybe_collector - INFO - Getting token details for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting token report: Request failed: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/report\n",
      "Error getting price history: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926338&timeEnd=1745531138 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A74D0>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error getting liquidity data: 'RugCheckCollector' object has no attribute 'get_token_liquidity_history'\n",
      "Insufficient data for launch analysis\n",
      "\n",
      "Analyzing launch metrics for JTO (jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:39,084 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A4F20>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:39,085 - rugcheck_collector - INFO - Getting token report for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n",
      "2025-04-25 00:45:39,085 - rugcheck_collector - INFO - Getting token report for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n",
      "2025-04-25 00:45:39,244 - rugcheck_collector - ERROR - Failed to make API request: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/report\n",
      "2025-04-25 00:45:39,244 - vybe_collector - INFO - Getting price OHLCV for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n",
      "2025-04-25 00:45:39,244 - rugcheck_collector - ERROR - Failed to make API request: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/report\n",
      "2025-04-25 00:45:39,244 - vybe_collector - INFO - Getting price OHLCV for jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting token details: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A4F20>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error getting token report: Request failed: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:39,419 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926339&timeEnd=1745531139 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0B3C20>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:39,420 - vybe_collector - INFO - Getting token details for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n",
      "2025-04-25 00:45:39,420 - vybe_collector - INFO - Getting token details for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting price history: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926339&timeEnd=1745531139 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0B3C20>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error getting liquidity data: 'RugCheckCollector' object has no attribute 'get_token_liquidity_history'\n",
      "Insufficient data for launch analysis\n",
      "\n",
      "Analyzing launch metrics for CROWN (FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:39,754 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0B0AD0>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-04-25 00:45:39,754 - rugcheck_collector - INFO - Getting token report for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n",
      "2025-04-25 00:45:39,754 - rugcheck_collector - INFO - Getting token report for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n",
      "2025-04-25 00:45:39,912 - rugcheck_collector - ERROR - Failed to make API request: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/report\n",
      "2025-04-25 00:45:39,913 - vybe_collector - INFO - Getting price OHLCV for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n",
      "2025-04-25 00:45:39,912 - rugcheck_collector - ERROR - Failed to make API request: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/report\n",
      "2025-04-25 00:45:39,913 - vybe_collector - INFO - Getting price OHLCV for FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting token details: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/token/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0B0AD0>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error getting token report: Request failed: 401 Client Error: Unauthorized for url: https://api.rugcheck.xyz/v1/tokens/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 00:45:40,087 - vybe_collector - ERROR - Failed to make API request: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926339&timeEnd=1745531139 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A6360>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting price history: Request failed: HTTPSConnectionPool(host='api.vybe.io', port=443): Max retries exceeded with url: /v1/price/FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2/token-ohlcv?limit=100&page=1&resolution=1h&timeStart=1744926339&timeEnd=1745531139 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029DED0A6360>: Failed to resolve 'api.vybe.io' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Error getting liquidity data: 'RugCheckCollector' object has no attribute 'get_token_liquidity_history'\n",
      "Insufficient data for launch analysis\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze ICO launch metrics\n",
    "def analyze_launch_metrics(mint_address, token_symbol):\n",
    "    print(f\"\\nAnalyzing launch metrics for {token_symbol} ({mint_address})\")\n",
    "    \n",
    "    # Get token details\n",
    "    try:\n",
    "        token_details = vybe.get_token_details(mint_address)\n",
    "        print(f\"Retrieved token details for {token_details.get('name', 'Unknown')} ({token_details.get('symbol', '???')})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token details: {e}\")\n",
    "        token_details = {\"symbol\": token_symbol}\n",
    "    \n",
    "    # Get token creator info from RugCheck\n",
    "    try:\n",
    "        token_report = rugcheck.get_token_report(mint_address)\n",
    "        creator_address = token_report.get(\"creator\")\n",
    "        if creator_address:\n",
    "            print(f\"Creator address: {creator_address}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token report: {e}\")\n",
    "        token_report = {}\n",
    "        creator_address = None\n",
    "    \n",
    "    # Get token price history to analyze launch\n",
    "    try:\n",
    "        # Get data for the first week after launch\n",
    "        launch_date = token_report.get(\"createdAt\")\n",
    "        if launch_date:\n",
    "            launch_timestamp = int(datetime.strptime(launch_date, \"%Y-%m-%dT%H:%M:%S.%fZ\").timestamp())\n",
    "            end_timestamp = launch_timestamp + (7 * 24 * 60 * 60)  # 7 days after launch\n",
    "        else:\n",
    "            # If no launch date, use the last 7 days\n",
    "            end_timestamp = int(datetime.now().timestamp())\n",
    "            launch_timestamp = end_timestamp - (7 * 24 * 60 * 60)\n",
    "        \n",
    "        price_data = vybe.get_token_price_ohlcv(\n",
    "            mint_address, \n",
    "            resolution=\"1h\", \n",
    "            time_start=launch_timestamp,\n",
    "            time_end=end_timestamp\n",
    "        )\n",
    "        price_history = price_data.get(\"data\", [])\n",
    "        print(f\"Retrieved {len(price_history)} price data points\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting price history: {e}\")\n",
    "        price_history = []\n",
    "    \n",
    "    # Get launch liquidity data\n",
    "    try:\n",
    "        liquidity_data = rugcheck.get_token_liquidity_history(mint_address)\n",
    "        print(f\"Retrieved liquidity history with {len(liquidity_data)} data points\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting liquidity data: {e}\")\n",
    "        liquidity_data = []\n",
    "    \n",
    "    # Calculate launch metrics\n",
    "    if price_history:\n",
    "        launch_metrics = calculate_ico_metrics(price_history, liquidity_data)\n",
    "        \n",
    "        # Display launch metrics\n",
    "        print(\"\\nLaunch Metrics:\")\n",
    "        if \"initial_price\" in launch_metrics:\n",
    "            print(f\"- Initial price: ${launch_metrics['initial_price']:.6f}\")\n",
    "        if \"price_peak\" in launch_metrics:\n",
    "            print(f\"- Peak price: ${launch_metrics['price_peak']:.6f}\")\n",
    "        if \"price_7d\" in launch_metrics:\n",
    "            print(f\"- 7-day price: ${launch_metrics['price_7d']:.6f}\")\n",
    "        if \"price_change_pct\" in launch_metrics:\n",
    "            print(f\"- 7-day price change: {launch_metrics['price_change_pct']:.2f}%\")\n",
    "        if \"initial_liquidity\" in launch_metrics:\n",
    "            print(f\"- Initial liquidity: ${launch_metrics['initial_liquidity']:.2f}\")\n",
    "        if \"liquidity_change_pct\" in launch_metrics:\n",
    "            print(f\"- 7-day liquidity change: {launch_metrics['liquidity_change_pct']:.2f}%\")\n",
    "        \n",
    "        # Plot price action\n",
    "        price_df = pd.DataFrame(price_history)\n",
    "        if not price_df.empty and \"timestamp\" in price_df and \"close\" in price_df:\n",
    "            price_df[\"datetime\"] = pd.to_datetime(price_df[\"timestamp\"], unit=\"s\")\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(price_df[\"datetime\"], price_df[\"close\"], 'b-')\n",
    "            plt.title(f\"{token_details.get('symbol', token_symbol)} Price Since Launch\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Price (USD)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Volume chart\n",
    "            if \"volume\" in price_df:\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.bar(price_df[\"datetime\"], price_df[\"volume\"], color='g', alpha=0.7)\n",
    "                plt.title(f\"{token_details.get('symbol', token_symbol)} Trading Volume Since Launch\")\n",
    "                plt.xlabel(\"Date\")\n",
    "                plt.ylabel(\"Volume (USD)\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        return {\n",
    "            \"token_symbol\": token_symbol,\n",
    "            \"mint_address\": mint_address,\n",
    "            \"creator_address\": creator_address,\n",
    "            \"launch_metrics\": launch_metrics,\n",
    "            \"price_history\": price_df.to_dict('records') if not price_df.empty else []\n",
    "        }\n",
    "    else:\n",
    "        print(\"Insufficient data for launch analysis\")\n",
    "        return {\n",
    "            \"token_symbol\": token_symbol,\n",
    "            \"mint_address\": mint_address,\n",
    "            \"creator_address\": creator_address\n",
    "        }\n",
    "\n",
    "# Analyze launch metrics for each selected token\n",
    "launch_results = {}\n",
    "for token_symbol, mint_address in selected_tokens:\n",
    "    launch_results[mint_address] = analyze_launch_metrics(mint_address, token_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577de9c",
   "metadata": {},
   "source": [
    "## Analyze Team Wallet Activity\n",
    "\n",
    "Let's analyze the activity of team wallets to detect potential red flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd99258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing team wallets for PYTH (HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB)\n",
      "Creator address not available\n",
      "No team wallets identified from distribution analysis\n",
      "No addresses available for team wallet analysis\n",
      "\n",
      "Analyzing team wallets for JTO (jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY)\n",
      "Creator address not available\n",
      "No team wallets identified from distribution analysis\n",
      "No addresses available for team wallet analysis\n",
      "\n",
      "Analyzing team wallets for CROWN (FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2)\n",
      "Creator address not available\n",
      "No team wallets identified from distribution analysis\n",
      "No addresses available for team wallet analysis\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze team wallet activity\n",
    "def analyze_team_wallets(mint_address, token_symbol):\n",
    "    print(f\"\\nAnalyzing team wallets for {token_symbol} ({mint_address})\")\n",
    "    \n",
    "    # Get creator address from launch results\n",
    "    creator_address = launch_results.get(mint_address, {}).get(\"creator_address\")\n",
    "    if not creator_address:\n",
    "        print(\"Creator address not available\")\n",
    "    else:\n",
    "        print(f\"Analyzing activity of creator address: {creator_address}\")\n",
    "    \n",
    "    # Get team wallets from distribution results\n",
    "    team_wallets = distribution_results.get(mint_address, {})\n",
    "    team_wallets = team_wallets.get(\"team_wallets\", []) if team_wallets else []\n",
    "    if not team_wallets:\n",
    "        print(\"No team wallets identified from distribution analysis\")\n",
    "    else:\n",
    "        print(f\"Found {len(team_wallets)} team wallets for analysis\")\n",
    "    \n",
    "    # Combine creator address and team wallets for analysis\n",
    "    analysis_addresses = [w[\"address\"] for w in team_wallets]\n",
    "    if creator_address and creator_address not in analysis_addresses:\n",
    "        analysis_addresses.append(creator_address)\n",
    "    \n",
    "    if not analysis_addresses:\n",
    "        print(\"No addresses available for team wallet analysis\")\n",
    "        return None\n",
    "    \n",
    "    team_activity = {}\n",
    "    for address in analysis_addresses:\n",
    "        print(f\"\\nAnalyzing activity of team wallet: {address}\")\n",
    "        \n",
    "        # Get transaction history\n",
    "        try:\n",
    "            tx_history = helius.fetch_transaction_history(address, limit=100)\n",
    "            print(f\"Fetched {len(tx_history)} transactions for team wallet\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transaction history: {e}\")\n",
    "            tx_history = []\n",
    "        \n",
    "        # Get token transfers specific to the project token\n",
    "        try:\n",
    "            token_transfers = helius.analyze_token_transfers(address, limit=100)\n",
    "            # Filter for the specific token we're analyzing\n",
    "            token_transfers = [tx for tx in token_transfers if tx.get(\"mint\") == mint_address]\n",
    "            print(f\"Fetched {len(token_transfers)} token transfers related to project token\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing token transfers: {e}\")\n",
    "            token_transfers = []\n",
    "        \n",
    "        # Analyze team wallet behavior\n",
    "        if token_transfers:\n",
    "            # Calculate total outflows/inflows of project tokens\n",
    "            outflows = [tx for tx in token_transfers if tx.get(\"sender_address\") == address]\n",
    "            inflows = [tx for tx in token_transfers if tx.get(\"receiver_address\") == address]\n",
    "            \n",
    "            total_outflow = sum(tx.get(\"amount\", 0) for tx in outflows)\n",
    "            total_inflow = sum(tx.get(\"amount\", 0) for tx in inflows)\n",
    "            net_flow = total_inflow - total_outflow\n",
    "            \n",
    "            print(f\"Total token outflow: {total_outflow:,.2f}\")\n",
    "            print(f\"Total token inflow: {total_inflow:,.2f}\")\n",
    "            print(f\"Net token flow: {net_flow:,.2f}\")\n",
    "            \n",
    "            # Identify large outflows\n",
    "            large_outflows = [tx for tx in outflows if tx.get(\"amount\", 0) > (total_inflow * 0.1)]\n",
    "            if large_outflows:\n",
    "                print(f\"Found {len(large_outflows)} large token outflows (>10% of inflows)\")\n",
    "            \n",
    "            # Calculate selling pressure\n",
    "            selling_pressure = total_outflow / total_inflow if total_inflow > 0 else 0\n",
    "            print(f\"Selling pressure: {selling_pressure:.2f} (>0.5 may indicate dumping)\")\n",
    "            \n",
    "            # Record analysis results\n",
    "            team_activity[address] = {\n",
    "                \"transaction_count\": len(tx_history),\n",
    "                \"token_transfers_count\": len(token_transfers),\n",
    "                \"total_outflow\": total_outflow,\n",
    "                \"total_inflow\": total_inflow,\n",
    "                \"net_flow\": net_flow,\n",
    "                \"selling_pressure\": selling_pressure,\n",
    "                \"large_outflows\": len(large_outflows),\n",
    "                \"is_dumping\": selling_pressure > 0.5 and net_flow < 0\n",
    "            }\n",
    "        else:\n",
    "            print(\"No token transfers found for analysis\")\n",
    "            team_activity[address] = {\n",
    "                \"transaction_count\": len(tx_history),\n",
    "                \"token_transfers_count\": 0\n",
    "            }\n",
    "    \n",
    "    # Summarize team activity\n",
    "    dumping_wallets = [addr for addr, data in team_activity.items() if data.get(\"is_dumping\", False)]\n",
    "    if dumping_wallets:\n",
    "        print(f\"\\n⚠️ Warning: {len(dumping_wallets)} team wallets show signs of dumping tokens\")\n",
    "        for addr in dumping_wallets:\n",
    "            print(f\"- {addr} (selling pressure: {team_activity[addr]['selling_pressure']:.2f})\")\n",
    "    else:\n",
    "        print(\"\\n✅ No team wallets show signs of dumping tokens\")\n",
    "    \n",
    "    return {\n",
    "        \"token_symbol\": token_symbol,\n",
    "        \"mint_address\": mint_address,\n",
    "        \"team_wallets\": analysis_addresses,\n",
    "        \"team_activity\": team_activity,\n",
    "        \"dumping_wallets\": dumping_wallets,\n",
    "        \"dumping_detected\": len(dumping_wallets) > 0\n",
    "    }\n",
    "\n",
    "# Analyze team wallet activity for each token\n",
    "team_results = {}\n",
    "for token_symbol, mint_address in selected_tokens:\n",
    "    team_results[mint_address] = analyze_team_wallets(mint_address, token_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b039a",
   "metadata": {},
   "source": [
    "## Calculate ICO Risk Scores\n",
    "\n",
    "Let's calculate comprehensive risk scores for each ICO based on our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcae35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating risk score for PYTH (HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3HFnNnfmj8UrAB)\n",
      "Risk score: 0.0% (Low Risk)\n",
      "Top risk factors:\n",
      "\n",
      "Calculating risk score for JTO (jtojtomepa8beP8AuQc6eXt5FriJwTMt8qPucZNEBxY)\n",
      "Risk score: 0.0% (Low Risk)\n",
      "Top risk factors:\n",
      "\n",
      "Calculating risk score for CROWN (FfmxuKM3dxhJuosGn1S4KKgVGTxj5FcVWHf9Rs4zC5K2)\n",
      "Risk score: 0.0% (Low Risk)\n",
      "Top risk factors:\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate ICO risk score\n",
    "def calculate_ico_risk_score(mint_address, token_symbol):\n",
    "    print(f\"\\nCalculating risk score for {token_symbol} ({mint_address})\")\n",
    "    \n",
    "    # Get data from previous analyses\n",
    "    distribution_data = distribution_results.get(mint_address, {})\n",
    "    launch_data = launch_results.get(mint_address, {})\n",
    "    team_data = team_results.get(mint_address, {})\n",
    "    \n",
    "    # Initialize risk score components\n",
    "    risk_factors = []\n",
    "    total_score = 0\n",
    "    max_score = 0\n",
    "    \n",
    "    # 1. Distribution risk factors (max 30 points)\n",
    "    if distribution_data:\n",
    "        max_score += 30\n",
    "        \n",
    "        # Top holder concentration risk (max 15 points)\n",
    "        top1_pct = distribution_data.get(\"top1_pct\", 0)\n",
    "        if top1_pct > 50:  # Very high risk if >50%\n",
    "            concentration_score = 15\n",
    "        elif top1_pct > 30:  # High risk if >30%\n",
    "            concentration_score = 10\n",
    "        elif top1_pct > 15:  # Medium risk if >15%\n",
    "            concentration_score = 5\n",
    "        else:  # Low risk\n",
    "            concentration_score = 0\n",
    "        \n",
    "        total_score += concentration_score\n",
    "        risk_factors.append({\n",
    "            \"name\": \"Token concentration\",\n",
    "            \"description\": f\"Top holder owns {top1_pct:.1f}% of supply\",\n",
    "            \"score\": concentration_score,\n",
    "            \"max_score\": 15\n",
    "        })\n",
    "        \n",
    "        # Team wallet holdings risk (max 15 points)\n",
    "        team_pct = distribution_data.get(\"team_wallets_pct\", 0)\n",
    "        if team_pct > 40:  # Very high risk if >40%\n",
    "            team_holding_score = 15\n",
    "        elif team_pct > 25:  # High risk if >25%\n",
    "            team_holding_score = 10\n",
    "        elif team_pct > 15:  # Medium risk if >15%\n",
    "            team_holding_score = 5\n",
    "        else:  # Low risk\n",
    "            team_holding_score = 0\n",
    "        \n",
    "        total_score += team_holding_score\n",
    "        if team_pct > 0:\n",
    "            risk_factors.append({\n",
    "                \"name\": \"Team holdings\",\n",
    "                \"description\": f\"Team wallets hold {team_pct:.1f}% of supply\",\n",
    "                \"score\": team_holding_score,\n",
    "                \"max_score\": 15\n",
    "            })\n",
    "    \n",
    "    # 2. Launch metrics risk factors (max 40 points)\n",
    "    if launch_data and \"launch_metrics\" in launch_data:\n",
    "        max_score += 40\n",
    "        launch_metrics = launch_data[\"launch_metrics\"]\n",
    "        \n",
    "        # Initial liquidity risk (max 20 points)\n",
    "        initial_liquidity = launch_metrics.get(\"initial_liquidity\", 0)\n",
    "        if initial_liquidity < 1000:  # Very high risk if <$1k\n",
    "            liquidity_score = 20\n",
    "        elif initial_liquidity < 5000:  # High risk if <$5k\n",
    "            liquidity_score = 15\n",
    "        elif initial_liquidity < 20000:  # Medium risk if <$20k\n",
    "            liquidity_score = 10\n",
    "        elif initial_liquidity < 50000:  # Low-medium risk if <$50k\n",
    "            liquidity_score = 5\n",
    "        else:  # Low risk if >$50k\n",
    "            liquidity_score = 0\n",
    "        \n",
    "        total_score += liquidity_score\n",
    "        risk_factors.append({\n",
    "            \"name\": \"Initial liquidity\",\n",
    "            \"description\": f\"${initial_liquidity:,.2f} initial liquidity\",\n",
    "            \"score\": liquidity_score,\n",
    "            \"max_score\": 20\n",
    "        })\n",
    "        \n",
    "        # Price volatility risk (max 20 points)\n",
    "        price_change = launch_metrics.get(\"price_change_pct\", 0)\n",
    "        if price_change < -50:  # Very high risk if dropped >50%\n",
    "            price_score = 20\n",
    "        elif price_change < -25:  # High risk if dropped >25%\n",
    "            price_score = 15\n",
    "        elif price_change > 1000:  # Medium-high risk if pumped >1000% (potential pump & dump)\n",
    "            price_score = 10\n",
    "        elif price_change < 0:  # Medium risk if any price drop\n",
    "            price_score = 5\n",
    "        else:  # Low risk\n",
    "            price_score = 0\n",
    "        \n",
    "        total_score += price_score\n",
    "        risk_factors.append({\n",
    "            \"name\": \"Price action\",\n",
    "            \"description\": f\"{price_change:.1f}% price change in first week\",\n",
    "            \"score\": price_score,\n",
    "            \"max_score\": 20\n",
    "        })\n",
    "    \n",
    "    # 3. Team activity risk factors (max 30 points)\n",
    "    if team_data:\n",
    "        max_score += 30\n",
    "        \n",
    "        # Team token dumping risk (max 30 points)\n",
    "        dumping_detected = team_data.get(\"dumping_detected\", False)\n",
    "        dumping_wallets = team_data.get(\"dumping_wallets\", [])\n",
    "        team_activity = team_data.get(\"team_activity\", {})\n",
    "        \n",
    "        # Calculate average selling pressure\n",
    "        selling_pressures = [data.get(\"selling_pressure\", 0) for addr, data in team_activity.items() \n",
    "                           if \"selling_pressure\" in data]\n",
    "        avg_selling_pressure = sum(selling_pressures) / len(selling_pressures) if selling_pressures else 0\n",
    "        \n",
    "        if dumping_detected and avg_selling_pressure > 0.8:  # Very high risk\n",
    "            dumping_score = 30\n",
    "        elif dumping_detected and avg_selling_pressure > 0.5:  # High risk\n",
    "            dumping_score = 20\n",
    "        elif avg_selling_pressure > 0.3:  # Medium risk\n",
    "            dumping_score = 10\n",
    "        else:  # Low risk\n",
    "            dumping_score = 0\n",
    "        \n",
    "        total_score += dumping_score\n",
    "        risk_description = f\"Team selling pressure: {avg_selling_pressure:.2f}\"\n",
    "        if dumping_detected:\n",
    "            risk_description += f\" ({len(dumping_wallets)} wallets dumping)\"\n",
    "        \n",
    "        risk_factors.append({\n",
    "            \"name\": \"Team token selling\",\n",
    "            \"description\": risk_description,\n",
    "            \"score\": dumping_score,\n",
    "            \"max_score\": 30\n",
    "        })\n",
    "    \n",
    "    # Calculate final risk percentage (0-100%)\n",
    "    risk_percentage = (total_score / max_score * 100) if max_score > 0 else 0\n",
    "    \n",
    "    # Determine risk level\n",
    "    if risk_percentage >= 75:\n",
    "        risk_level = \"Very High Risk\"\n",
    "    elif risk_percentage >= 50:\n",
    "        risk_level = \"High Risk\"\n",
    "    elif risk_percentage >= 25:\n",
    "        risk_level = \"Medium Risk\"\n",
    "    else:\n",
    "        risk_level = \"Low Risk\"\n",
    "    \n",
    "    print(f\"Risk score: {risk_percentage:.1f}% ({risk_level})\")\n",
    "    print(\"Top risk factors:\")\n",
    "    for factor in sorted(risk_factors, key=lambda x: x[\"score\"], reverse=True)[:3]:  # Top 3 factors\n",
    "        print(f\"- {factor['name']}: {factor['description']} (score: {factor['score']}/{factor['max_score']})\")\n",
    "    \n",
    "    return {\n",
    "        \"token_symbol\": token_symbol,\n",
    "        \"mint_address\": mint_address,\n",
    "        \"risk_score\": risk_percentage,\n",
    "        \"risk_level\": risk_level,\n",
    "        \"risk_factors\": risk_factors\n",
    "    }\n",
    "\n",
    "# Calculate risk scores for each token\n",
    "risk_results = {}\n",
    "for token_symbol, mint_address in selected_tokens:\n",
    "    risk_results[mint_address] = calculate_ico_risk_score(mint_address, token_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bad52",
   "metadata": {},
   "source": [
    "## Conclusions and Report Generation\n",
    "\n",
    "Let's summarize our findings and generate a comprehensive ICO analysis report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78871120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report generated and saved to ../../reports/ico_analysis_report_20250425_004540.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Solana ICO Analysis Report\n",
       "\n",
       "Generated on: 2025-04-25 00:45:40\n",
       "\n",
       "## Summary of Analyzed Tokens\n",
       "\n",
       "| Token | Risk Level | Risk Score | Initial Liquidity | Top Holder % | Team Dumping |\n",
       "| ----- | ---------- | ---------- | ----------------- | ------------ | ------------ |\n",
       "| PYTH | Low Risk | 0.0% | N/A | N/A | No |\n",
       "| JTO | Low Risk | 0.0% | N/A | N/A | No |\n",
       "| CROWN | Low Risk | 0.0% | N/A | N/A | No |\n",
       "\n",
       "## PYTH (HZ1JovNi...)\n",
       "\n",
       "### Risk Assessment\n",
       "\n",
       "- **Overall risk score**: 0.0%\n",
       "- **Risk level**: Low Risk\n",
       "\n",
       "#### Risk Factors\n",
       "\n",
       "- No specific risk factors identified\n",
       "\n",
       "### Token Distribution\n",
       "\n",
       "- Distribution data not available\n",
       "\n",
       "### Launch Metrics\n",
       "\n",
       "\n",
       "### Team Wallet Activity\n",
       "\n",
       "- Team wallet activity data not available\n",
       "\n",
       "## JTO (jtojtome...)\n",
       "\n",
       "### Risk Assessment\n",
       "\n",
       "- **Overall risk score**: 0.0%\n",
       "- **Risk level**: Low Risk\n",
       "\n",
       "#### Risk Factors\n",
       "\n",
       "- No specific risk factors identified\n",
       "\n",
       "### Token Distribution\n",
       "\n",
       "- Distribution data not available\n",
       "\n",
       "### Launch Metrics\n",
       "\n",
       "\n",
       "### Team Wallet Activity\n",
       "\n",
       "- Team wallet activity data not available\n",
       "\n",
       "## CROWN (FfmxuKM3...)\n",
       "\n",
       "### Risk Assessment\n",
       "\n",
       "- **Overall risk score**: 0.0%\n",
       "- **Risk level**: Low Risk\n",
       "\n",
       "#### Risk Factors\n",
       "\n",
       "- No specific risk factors identified\n",
       "\n",
       "### Token Distribution\n",
       "\n",
       "- Distribution data not available\n",
       "\n",
       "### Launch Metrics\n",
       "\n",
       "\n",
       "### Team Wallet Activity\n",
       "\n",
       "- Team wallet activity data not available\n",
       "\n",
       "## General Recommendations for ICO Investors\n",
       "\n",
       "1. **Check token distribution** - Be cautious of tokens where top holders control >50% of supply\n",
       "2. **Verify liquidity** - Higher liquidity (>$50k) reduces manipulation risk\n",
       "3. **Monitor team wallet activity** - Watch for team wallets selling large amounts early\n",
       "4. **Examine price action** - Extreme volatility may indicate market manipulation\n",
       "5. **Look for locked liquidity** - Tokens with locked liquidity are generally safer\n",
       "6. **Diversify investments** - Never put all your funds into a single ICO"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate ICO analysis report\n",
    "def generate_ico_report():\n",
    "    report = [\"# Solana ICO Analysis Report\\n\"]\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    \n",
    "    # Summary table of all analyzed tokens\n",
    "    report.append(\"## Summary of Analyzed Tokens\\n\")\n",
    "    report.append(\"| Token | Risk Level | Risk Score | Initial Liquidity | Top Holder % | Team Dumping |\")\n",
    "    report.append(\"| ----- | ---------- | ---------- | ----------------- | ------------ | ------------ |\")\n",
    "    \n",
    "    for token_symbol, mint_address in selected_tokens:\n",
    "        # Get data for token\n",
    "        risk_data = risk_results.get(mint_address, {})\n",
    "        distribution_data = distribution_results.get(mint_address, {}) or {}\n",
    "        launch_data = launch_results.get(mint_address, {}) or {}\n",
    "        launch_metrics = launch_data.get(\"launch_metrics\", {}) or {}\n",
    "        team_data = team_results.get(mint_address, {}) or {}\n",
    "        \n",
    "        # Format table row\n",
    "        risk_level = risk_data.get(\"risk_level\", \"Unknown\") if risk_data else \"Unknown\"\n",
    "        risk_score = f\"{risk_data.get('risk_score', 0):.1f}%\" if risk_data and \"risk_score\" in risk_data else \"N/A\"\n",
    "        liquidity = f\"${launch_metrics.get('initial_liquidity', 0):,.2f}\" if launch_metrics and \"initial_liquidity\" in launch_metrics else \"N/A\"\n",
    "        top_holder = f\"{distribution_data.get('top1_pct', 0):.1f}%\" if distribution_data and \"top1_pct\" in distribution_data else \"N/A\"\n",
    "        team_dumping = \"Yes\" if team_data and team_data.get(\"dumping_detected\", False) else \"No\"\n",
    "        \n",
    "        report.append(f\"| {token_symbol} | {risk_level} | {risk_score} | {liquidity} | {top_holder} | {team_dumping} |\")\n",
    "    \n",
    "    # Detailed analysis for each token\n",
    "    for token_symbol, mint_address in selected_tokens:\n",
    "        report.append(f\"\\n## {token_symbol} ({mint_address[:8]}...)\\n\")\n",
    "        \n",
    "        # Risk assessment\n",
    "        risk_data = risk_results.get(mint_address, {})\n",
    "        if risk_data is not None and isinstance(risk_data, dict):\n",
    "            report.append(f\"### Risk Assessment\\n\")\n",
    "            report.append(f\"- **Overall risk score**: {risk_data.get('risk_score', 0):.1f}%\")\n",
    "            report.append(f\"- **Risk level**: {risk_data.get('risk_level', 'Unknown')}\\n\")\n",
    "            \n",
    "            report.append(\"#### Risk Factors\\n\")\n",
    "            risk_factors = risk_data.get(\"risk_factors\", [])\n",
    "            if risk_factors:\n",
    "                for factor in sorted(risk_factors, key=lambda x: x[\"score\"], reverse=True):\n",
    "                    report.append(f\"- **{factor['name']}**: {factor['description']} (score: {factor['score']}/{factor['max_score']})\")\n",
    "            else:\n",
    "                report.append(\"- No specific risk factors identified\")\n",
    "        \n",
    "        # Token distribution\n",
    "        distribution_data = distribution_results.get(mint_address, {})\n",
    "        if distribution_data is not None and isinstance(distribution_data, dict):\n",
    "            report.append(f\"\\n### Token Distribution\\n\")\n",
    "            report.append(f\"- Total holders: {distribution_data.get('holders_count', 'Unknown')}\")\n",
    "            report.append(f\"- Top holder percentage: {distribution_data.get('top1_pct', 0):.1f}%\")\n",
    "            report.append(f\"- Top 5 holders percentage: {distribution_data.get('top5_pct', 0):.1f}%\")\n",
    "            report.append(f\"- Top 10 holders percentage: {distribution_data.get('top10_pct', 0):.1f}%\")\n",
    "            \n",
    "            if \"gini_coefficient\" in distribution_data and distribution_data[\"gini_coefficient\"] is not None:\n",
    "                report.append(f\"- Gini coefficient: {distribution_data['gini_coefficient']:.4f} (0=equal distribution, 1=completely unequal)\")\n",
    "            \n",
    "            team_wallets = distribution_data.get(\"team_wallets\", [])\n",
    "            if team_wallets:\n",
    "                report.append(f\"- Team wallets control: {distribution_data.get('team_wallets_pct', 0):.1f}% of supply\")\n",
    "                report.append(\"\\n#### Identified Team Wallets\\n\")\n",
    "                for wallet in team_wallets:\n",
    "                    report.append(f\"- `{wallet['address']}`: {wallet['percentage']:.1f}% of supply (confidence: {wallet['confidence']:.2f})\")\n",
    "        else:\n",
    "            report.append(f\"\\n### Token Distribution\\n\")\n",
    "            report.append(\"- Distribution data not available\")\n",
    "        \n",
    "        # Launch metrics\n",
    "        launch_data = launch_results.get(mint_address, {})\n",
    "        launch_metrics = launch_data.get(\"launch_metrics\", {}) if launch_data else {}\n",
    "        if launch_metrics is not None and isinstance(launch_metrics, dict):\n",
    "            report.append(f\"\\n### Launch Metrics\\n\")\n",
    "            if \"initial_price\" in launch_metrics:\n",
    "                report.append(f\"- Initial price: ${launch_metrics['initial_price']:.6f}\")\n",
    "            if \"price_peak\" in launch_metrics:\n",
    "                report.append(f\"- Peak price: ${launch_metrics['price_peak']:.6f}\")\n",
    "            if \"price_7d\" in launch_metrics:\n",
    "                report.append(f\"- 7-day price: ${launch_metrics['price_7d']:.6f}\")\n",
    "            if \"price_change_pct\" in launch_metrics:\n",
    "                report.append(f\"- 7-day price change: {launch_metrics['price_change_pct']:.2f}%\")\n",
    "            if \"initial_liquidity\" in launch_metrics:\n",
    "                report.append(f\"- Initial liquidity: ${launch_metrics['initial_liquidity']:.2f}\")\n",
    "            if \"liquidity_change_pct\" in launch_metrics:\n",
    "                report.append(f\"- 7-day liquidity change: {launch_metrics['liquidity_change_pct']:.2f}%\")\n",
    "        else:\n",
    "            report.append(f\"\\n### Launch Metrics\\n\")\n",
    "            report.append(\"- Launch metrics data not available\")\n",
    "        \n",
    "        # Team wallet activity\n",
    "        team_data = team_results.get(mint_address, {})\n",
    "        if team_data is not None and isinstance(team_data, dict):\n",
    "            report.append(f\"\\n### Team Wallet Activity\\n\")\n",
    "            \n",
    "            if team_data.get(\"dumping_detected\", False):\n",
    "                report.append(f\"⚠️ **Warning: Team wallets showing signs of dumping tokens**\\n\")\n",
    "            else:\n",
    "                report.append(f\"✅ No evidence of team dumping detected\\n\")\n",
    "            \n",
    "            team_activity = team_data.get(\"team_activity\", {})\n",
    "            if team_activity:\n",
    "                for addr, activity in team_activity.items():\n",
    "                    if isinstance(activity, dict) and \"selling_pressure\" in activity:\n",
    "                        status = \"🔴 Dumping\" if activity.get(\"is_dumping\", False) else \"🟢 Normal\"\n",
    "                        report.append(f\"- `{addr[:8]}...`: {status} (selling pressure: {activity['selling_pressure']:.2f}, net flow: {activity['net_flow']:,.0f} tokens)\")\n",
    "        else:\n",
    "            report.append(f\"\\n### Team Wallet Activity\\n\")\n",
    "            report.append(\"- Team wallet activity data not available\")\n",
    "    \n",
    "    # General recommendations\n",
    "    report.append(\"\\n## General Recommendations for ICO Investors\\n\")\n",
    "    report.append(\"1. **Check token distribution** - Be cautious of tokens where top holders control >50% of supply\")\n",
    "    report.append(\"2. **Verify liquidity** - Higher liquidity (>$50k) reduces manipulation risk\")\n",
    "    report.append(\"3. **Monitor team wallet activity** - Watch for team wallets selling large amounts early\")\n",
    "    report.append(\"4. **Examine price action** - Extreme volatility may indicate market manipulation\")\n",
    "    report.append(\"5. **Look for locked liquidity** - Tokens with locked liquidity are generally safer\")\n",
    "    report.append(\"6. **Diversify investments** - Never put all your funds into a single ICO\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and save the report\n",
    "report_content = generate_ico_report()\n",
    "\n",
    "# Save report to file\n",
    "report_path = f\"../../reports/ico_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "os.makedirs(\"../../reports\", exist_ok=True)\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"\\nReport generated and saved to {report_path}\")\n",
    "\n",
    "# Display report in notebook\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(report_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
