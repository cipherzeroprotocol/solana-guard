{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da258df3",
   "metadata": {},
   "source": [
    "# Token Insider Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to use SolanaGuard to analyze tokens for insider trading patterns, team wallet identification, and token creator behavior.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Token insider analysis focuses on identifying patterns that may indicate:  \n",
    "1. **Team wallets**: Addresses controlled by project team members  \n",
    "2. **Insider trading**: Trading based on non-public information  \n",
    "3. **Rug pull preparation**: Token creators preparing to abandon the project  \n",
    "4. **Wash trading**: Creating fake trading volume  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf38b02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'collectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Import SolanaGuard modules\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrugcheck_collector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RugCheckCollector\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvybe_collector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VybeCollector\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelius_collector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HeliusCollector\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'collectors'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('../..')\n",
    "\n",
    "# Import SolanaGuard modules\n",
    "from data_collection.collectors.helius_collector import HeliusCollector\n",
    "from data_collection.collectors.rugcheck_collector import RugCheckCollector\n",
    "from data_collection.collectors.vybe_collector import VybeCollector\n",
    "from utils.graph_utils import build_token_insider_graph\n",
    "from utils.risk_scoring import calculate_token_risk\n",
    "from utils.visualization import visualize_token_activity, visualize_risk_score\n",
    "\n",
    "# Configure plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5aa3b",
   "metadata": {},
   "source": [
    "## Initialize API Collectors\n",
    "\n",
    "First, we initialize the necessary API collectors to gather token data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collectors\n",
    "rugcheck = RugCheckCollector()\n",
    "vybe = VybeCollector()\n",
    "helius = HeliusCollector()\n",
    "\n",
    "print(\"Collectors initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13070652",
   "metadata": {},
   "source": [
    "## Identify Suspicious Tokens\n",
    "\n",
    "Let's start by identifying potentially suspicious tokens from trending data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the API doesn't work, we can specify known tokens of interest directly\n",
    "predefined_suspicious_tokens = [\n",
    "    {\n",
    "        \"mint\": \"7dHbWXmci3dT8UFYWYZweBLXgycu7Y3iL6trKn1Y7ARj\", # Bonk\n",
    "        \"name\": \"Bonk\",\n",
    "        \"symbol\": \"BONK\",\n",
    "        \"risk_score\": 70,\n",
    "        \"risks\": [\"High concentration\", \"Team wallet selling\"]\n",
    "    },\n",
    "    {\n",
    "        \"mint\": \"EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\", # USDC on Solana\n",
    "        \"name\": \"USD Coin\",\n",
    "        \"symbol\": \"USDC\",\n",
    "        \"risk_score\": 30,\n",
    "        \"risks\": [\"Centralized control\"]\n",
    "    },\n",
    "    {\n",
    "        \"mint\": \"4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R\", # Raydium\n",
    "        \"name\": \"Raydium\",\n",
    "        \"symbol\": \"RAY\",\n",
    "        \"risk_score\": 45,\n",
    "        \"risks\": [\"Concentration risk\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get trending tokens data\n",
    "try:\n",
    "    trending_tokens = rugcheck.get_trending_tokens()\n",
    "    print(f\"Retrieved {len(trending_tokens)} trending tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting trending tokens: {e}\")\n",
    "    trending_tokens = []\n",
    "    \n",
    "# If API call fails, use our predefined tokens\n",
    "if not trending_tokens:\n",
    "    print(\"Using predefined token list as fallback\")\n",
    "    suspicious_tokens = predefined_suspicious_tokens\n",
    "else:\n",
    "    # Function to filter suspicious tokens\n",
    "    def filter_suspicious_tokens(tokens, max_tokens=10):\n",
    "        suspicious_tokens = []\n",
    "        \n",
    "        for token in tokens[:20]:  # Check top 20 tokens\n",
    "            token_mint = token.get(\"mint\")\n",
    "            if not token_mint:\n",
    "                continue\n",
    "                \n",
    "            # Get token summary report\n",
    "            try:\n",
    "                summary = rugcheck.get_token_report_summary(token_mint, cache_only=True)\n",
    "                \n",
    "                # Check if token has high risk score\n",
    "                if summary.get(\"score_normalised\", 0) > 60:\n",
    "                    suspicious_tokens.append({\n",
    "                        \"mint\": token_mint,\n",
    "                        \"name\": token.get(\"name\", \"Unknown\"),\n",
    "                        \"symbol\": token.get(\"symbol\", \"???\"),\n",
    "                        \"risk_score\": summary.get(\"score_normalised\", 0),\n",
    "                        \"risks\": summary.get(\"risks\", [])\n",
    "                    })\n",
    "                    \n",
    "                    if len(suspicious_tokens) >= max_tokens:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing token {token_mint}: {e}\")\n",
    "        \n",
    "        return suspicious_tokens\n",
    "\n",
    "    suspicious_tokens = filter_suspicious_tokens(trending_tokens)\n",
    "\n",
    "print(f\"\\nIdentified {len(suspicious_tokens)} suspicious tokens:\")\n",
    "for token in suspicious_tokens:\n",
    "    print(f\"- {token['symbol']} ({token['mint'][:10]}...): Risk score {token['risk_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391addb",
   "metadata": {},
   "source": [
    "## Analyze Token Creator Patterns\n",
    "\n",
    "Let's analyze the creator wallet patterns for these suspicious tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a90017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze token creator patterns\n",
    "def analyze_token_creator(token_mint):\n",
    "    print(f\"\\nAnalyzing creator for token: {token_mint}\")\n",
    "    \n",
    "    # Get full token report\n",
    "    try:\n",
    "        token_report = rugcheck.get_token_report(token_mint)\n",
    "        creator_address = token_report.get(\"creator\")\n",
    "        if not creator_address:\n",
    "            print(\"Creator address not found\")\n",
    "            return {}\n",
    "            \n",
    "        print(f\"Creator address: {creator_address}\")\n",
    "        \n",
    "        # Get creator's other tokens\n",
    "        creator_tokens = token_report.get(\"creatorTokens\", [])\n",
    "        print(f\"Creator has launched {len(creator_tokens)} tokens\")\n",
    "        \n",
    "        # Get token's metadata\n",
    "        token_meta = token_report.get(\"tokenMeta\", {})\n",
    "        token_name = token_meta.get(\"name\", \"Unknown\")\n",
    "        token_symbol = token_meta.get(\"symbol\", \"???\")\n",
    "        \n",
    "        # Get transaction history for creator\n",
    "        tx_history = helius.fetch_transaction_history(creator_address, limit=100)\n",
    "        print(f\"Fetched {len(tx_history)} transactions for creator\")\n",
    "        \n",
    "        # Get token transfers for creator\n",
    "        token_transfers = helius.analyze_token_transfers(creator_address, limit=100)\n",
    "        print(f\"Fetched {len(token_transfers)} token transfers for creator\")\n",
    "        \n",
    "        # Analyze other tokens from same creator\n",
    "        other_tokens_risk = []\n",
    "        for creator_token in creator_tokens:\n",
    "            other_mint = creator_token.get(\"mint\")\n",
    "            if other_mint and other_mint != token_mint:\n",
    "                try:\n",
    "                    other_summary = rugcheck.get_token_report_summary(other_mint, cache_only=True)\n",
    "                    other_tokens_risk.append({\n",
    "                        \"mint\": other_mint,\n",
    "                        \"risk_score\": other_summary.get(\"score_normalised\", 0),\n",
    "                        \"created_at\": creator_token.get(\"createdAt\"),\n",
    "                        \"market_cap\": creator_token.get(\"marketCap\", 0)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing other token {other_mint}: {e}\")\n",
    "        \n",
    "        # Calculate average risk score for creator's tokens\n",
    "        avg_risk = np.mean([t[\"risk_score\"] for t in other_tokens_risk]) if other_tokens_risk else 0\n",
    "        print(f\"Average risk score for creator's other tokens: {avg_risk:.2f}\")\n",
    "        \n",
    "        # Determine if creator has history of rug pulls\n",
    "        high_risk_tokens = [t for t in other_tokens_risk if t[\"risk_score\"] > 80]\n",
    "        has_rug_history = len(high_risk_tokens) > 0\n",
    "        if has_rug_history:\n",
    "            print(f\"Creator has {len(high_risk_tokens)} high-risk tokens (potential rug pulls)\")\n",
    "        \n",
    "        return {\n",
    "            \"token_mint\": token_mint,\n",
    "            \"token_name\": token_name,\n",
    "            \"token_symbol\": token_symbol,\n",
    "            \"creator_address\": creator_address,\n",
    "            \"creator_tokens_count\": len(creator_tokens),\n",
    "            \"other_tokens_risk\": other_tokens_risk,\n",
    "            \"avg_risk_score\": avg_risk,\n",
    "            \"has_rug_history\": has_rug_history,\n",
    "            \"tx_history\": tx_history,\n",
    "            \"token_transfers\": token_transfers\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token report: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Analyze creators for suspicious tokens\n",
    "creator_analyses = {}\n",
    "for token in suspicious_tokens:\n",
    "    creator_analyses[token[\"mint\"]] = analyze_token_creator(token[\"mint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080fbee",
   "metadata": {},
   "source": [
    "## Insider Network Analysis\n",
    "\n",
    "Now, let's analyze the insider networks for these tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a09110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze token insider network\n",
    "def analyze_token_insiders(token_mint):\n",
    "    print(f\"\\nAnalyzing insider network for token: {token_mint}\")\n",
    "    \n",
    "    # Get insider graph\n",
    "    try:\n",
    "        insider_graph_data = rugcheck.get_token_insider_graph(token_mint)\n",
    "        \n",
    "        if not insider_graph_data:\n",
    "            print(\"No insider graph data available\")\n",
    "            return {}\n",
    "        \n",
    "        # Build insider graph\n",
    "        graph = build_token_insider_graph(insider_graph_data)\n",
    "        \n",
    "        # Get insider metrics\n",
    "        node_count = len(graph.graph.nodes())\n",
    "        edge_count = len(graph.graph.edges())\n",
    "        print(f\"Insider graph has {node_count} nodes and {edge_count} edges\")\n",
    "        \n",
    "        # Get top holders\n",
    "        try:\n",
    "            token_report = rugcheck.get_token_report(token_mint)\n",
    "            top_holders = token_report.get(\"topHolders\", [])\n",
    "            print(f\"Retrieved {len(top_holders)} top holders\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting top holders: {e}\")\n",
    "            top_holders = []\n",
    "        \n",
    "        # Calculate holder concentration\n",
    "        top_holder_pct = top_holders[0].get(\"pct\", 0) if top_holders else 0\n",
    "        top5_pct = sum(h.get(\"pct\", 0) for h in top_holders[:5]) if len(top_holders) >= 5 else sum(h.get(\"pct\", 0) for h in top_holders)\n",
    "        \n",
    "        print(f\"Top holder controls {top_holder_pct:.2f}% of supply\")\n",
    "        print(f\"Top 5 holders control {top5_pct:.2f}% of supply\")\n",
    "        \n",
    "        # Get insider marked holders\n",
    "        insider_holders = [h for h in top_holders if h.get(\"insider\", False)]\n",
    "        insider_pct = sum(h.get(\"pct\", 0) for h in insider_holders)\n",
    "        \n",
    "        print(f\"Identified {len(insider_holders)} insider wallets controlling {insider_pct:.2f}% of supply\")\n",
    "        \n",
    "        # Analyze graph communities\n",
    "        communities = graph.identify_communities()\n",
    "        print(f\"Detected {len(communities)} communities in the insider network\")\n",
    "        \n",
    "        # Analyze token liquidity\n",
    "        markets = token_report.get(\"markets\", [])\n",
    "        total_liquidity = 0\n",
    "        liquidity_locked_pct = 0\n",
    "        \n",
    "        for market in markets:\n",
    "            lp_data = market.get(\"lp\", {})\n",
    "            if lp_data:\n",
    "                total_liquidity += lp_data.get(\"baseUSD\", 0) + lp_data.get(\"quoteUSD\", 0)\n",
    "                liquidity_locked_pct = max(liquidity_locked_pct, lp_data.get(\"lpLockedPct\", 0))\n",
    "        \n",
    "        print(f\"Total liquidity: ${total_liquidity:.2f}\")\n",
    "        print(f\"Liquidity locked: {liquidity_locked_pct:.2f}%\")\n",
    "        \n",
    "        # Identify suspicious patterns\n",
    "        suspicious_patterns = graph.detect_suspicious_patterns()\n",
    "        print(f\"Detected {len(suspicious_patterns)} suspicious patterns in the insider network\")\n",
    "        \n",
    "        for pattern in suspicious_patterns:\n",
    "            print(f\"- {pattern['type']}: {pattern['description']} (risk: {pattern['risk_score']})\")\n",
    "        \n",
    "        # Save graph for visualization\n",
    "        graph_json = graph.export_to_json()\n",
    "        with open(f\"../../data/output/insider_graph_{token_mint}.json\", \"w\") as f:\n",
    "            json.dump(graph_json, f)\n",
    "        \n",
    "        # Return insider analysis results\n",
    "        return {\n",
    "            \"token_mint\": token_mint,\n",
    "            \"node_count\": node_count,\n",
    "            \"edge_count\": edge_count,\n",
    "            \"top_holders\": top_holders,\n",
    "            \"top_holder_pct\": top_holder_pct,\n",
    "            \"top5_pct\": top5_pct,\n",
    "            \"insider_holders\": insider_holders,\n",
    "            \"insider_pct\": insider_pct,\n",
    "            \"communities\": communities,\n",
    "            \"total_liquidity\": total_liquidity,\n",
    "            \"liquidity_locked_pct\": liquidity_locked_pct,\n",
    "            \"suspicious_patterns\": suspicious_patterns,\n",
    "            \"graph\": graph\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing insider network: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Analyze insider networks for suspicious tokens\n",
    "insider_analyses = {}\n",
    "for token in suspicious_tokens:\n",
    "    insider_analyses[token[\"mint\"]] = analyze_token_insiders(token[\"mint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532dca9",
   "metadata": {},
   "source": [
    "## Token Activity Analysis\n",
    "\n",
    "Let's analyze token activity patterns to identify potential wash trading or price manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd52584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze token activity\n",
    "def analyze_token_activity(token_mint):\n",
    "    print(f\"\\nAnalyzing token activity for: {token_mint}\")\n",
    "    \n",
    "    # Get token details\n",
    "    try:\n",
    "        token_details = vybe.get_token_details(token_mint)\n",
    "        print(f\"Retrieved token details for {token_details.get('name', 'Unknown')} ({token_details.get('symbol', '???')})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token details: {e}\")\n",
    "        token_details = {}\n",
    "    \n",
    "    # Get token price history\n",
    "    try:\n",
    "        price_data = vybe.get_token_price_ohlcv(\n",
    "            token_mint, \n",
    "            resolution=\"1d\", \n",
    "            time_start=int((datetime.now() - timedelta(days=30)).timestamp()),\n",
    "            time_end=int(datetime.now().timestamp())\n",
    "        )\n",
    "        price_history = price_data.get(\"data\", [])\n",
    "        print(f\"Retrieved {len(price_history)} price data points\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting price history: {e}\")\n",
    "        price_history = []\n",
    "    \n",
    "    # Get token holders\n",
    "    try:\n",
    "        holders_data = vybe.get_token_top_holders(token_mint)\n",
    "        holders = holders_data.get(\"data\", [])\n",
    "        print(f\"Retrieved {len(holders)} token holders\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token holders: {e}\")\n",
    "        holders = []\n",
    "    \n",
    "    # Get token transfers\n",
    "    try:\n",
    "        transfers_data = vybe.get_token_transfers(\n",
    "            mint_address=token_mint,\n",
    "            time_start=int((datetime.now() - timedelta(days=7)).timestamp()),\n",
    "            time_end=int(datetime.now().timestamp()),\n",
    "            limit=500\n",
    "        )\n",
    "        transfers = transfers_data.get(\"data\", [])\n",
    "        print(f\"Retrieved {len(transfers)} token transfers\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting token transfers: {e}\")\n",
    "        transfers = []\n",
    "    \n",
    "    # Analyze transfer patterns for wash trading\n",
    "    wash_trading_indicators = 0\n",
    "    wash_trading_reasons = []\n",
    "    \n",
    "    if transfers and len(transfers) > 10:\n",
    "        # Check for circular transfers\n",
    "        transfer_pairs = set()\n",
    "        for tx in transfers:\n",
    "            sender = tx.get(\"sender_address\")\n",
    "            receiver = tx.get(\"receiver_address\")\n",
    "            \n",
    "            if sender and receiver:\n",
    "                if (receiver, sender) in transfer_pairs:\n",
    "                    wash_trading_indicators += 1\n",
    "                    wash_trading_reasons.append(f\"Circular transfer between {sender[:6]}... and {receiver[:6]}...\")\n",
    "                \n",
    "                transfer_pairs.add((sender, receiver))\n",
    "        \n",
    "        # Check for unusual timing patterns\n",
    "        if len(transfers) >= 3:\n",
    "            # Convert timestamps to datetime objects\n",
    "            timestamps = sorted([tx.get(\"block_time\", 0) for tx in transfers if tx.get(\"block_time\")])\n",
    "            \n",
    "            if len(timestamps) >= 3:\n",
    "                # Calculate time differences\n",
    "                time_diffs = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "                \n",
    "                # Check for extremely regular time intervals\n",
    "                if len(time_diffs) > 5:\n",
    "                    avg_diff = np.mean(time_diffs)\n",
    "                    std_diff = np.std(time_diffs)\n",
    "                    \n",
    "                    if std_diff / avg_diff < 0.1 and len(time_diffs) > 10:  # Very consistent timing\n",
    "                        wash_trading_indicators += 2\n",
    "                        wash_trading_reasons.append(f\"Extremely regular transaction timing (CV: {std_diff/avg_diff:.4f})\")\n",
    "    \n",
    "    # Check for unusual volume patterns in price data\n",
    "    if price_history and len(price_history) > 5:\n",
    "        volumes = [p.get(\"volume\", 0) for p in price_history]\n",
    "        \n",
    "        # Check for sudden volume spikes\n",
    "        for i in range(1, len(volumes)):\n",
    "            if volumes[i] > 5 * volumes[i-1] and volumes[i] > 0:\n",
    "                wash_trading_indicators += 1\n",
    "                wash_trading_reasons.append(f\"Sudden volume spike ({volumes[i]/volumes[i-1]:.1f}x increase)\")\n",
    "    \n",
    "    print(f\"Wash trading indicators: {wash_trading_indicators}\")\n",
    "    if wash_trading_reasons:\n",
    "        for reason in wash_trading_reasons[:3]:  # Show top 3 reasons\n",
    "            print(f\"- {reason}\")\n",
    "    \n",
    "    # Visualize token activity\n",
    "    token_data = {\n",
    "        \"token_mint\": token_mint,\n",
    "        \"token_name\": token_details.get(\"name\", \"Unknown\"),\n",
    "        \"token_symbol\": token_details.get(\"symbol\", \"???\"),\n",
    "        \"price_history\": price_history,\n",
    "        \"holder_data\": holders,\n",
    "        \"volume_data\": price_history  # Use the same data for volume\n",
    "    }\n",
    "    \n",
    "    # Add risk data if available\n",
    "    for token_info in suspicious_tokens:\n",
    "        if token_info[\"mint\"] == token_mint:\n",
    "            token_data[\"risk_score\"] = token_info[\"risk_score\"]\n",
    "            token_data[\"risk_level\"] = \"high\" if token_info[\"risk_score\"] > 75 else \"medium\"\n",
    "            break\n",
    "    \n",
    "    # Generate visualization\n",
    "    viz_file = visualize_token_activity(token_data, output_file=f\"../../data/visualizations/token_activity_{token_mint}.png\")\n",
    "    \n",
    "    return {\n",
    "        \"token_mint\": token_mint,\n",
    "        \"token_details\": token_details,\n",
    "        \"price_history\": price_history,\n",
    "        \"holders\": holders,\n",
    "        \"transfers\": transfers,\n",
    "        \"wash_trading_indicators\": wash_trading_indicators,\n",
    "        \"wash_trading_reasons\": wash_trading_reasons,\n",
    "        \"visualization_file\": viz_file\n",
    "    }\n",
    "\n",
    "# Analyze token activity for suspicious tokens\n",
    "activity_analyses = {}\n",
    "for token in suspicious_tokens:\n",
    "    activity_analyses[token[\"mint\"]] = analyze_token_activity(token[\"mint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04eae93",
   "metadata": {},
   "source": [
    "## Token Risk Assessment\n",
    "\n",
    "Let's calculate comprehensive risk scores for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive risk scores for each token\n",
    "token_risk_scores = {}\n",
    "\n",
    "for token in suspicious_tokens:\n",
    "    token_mint = token[\"mint\"]\n",
    "    \n",
    "    # Gather all data needed for risk calculation\n",
    "    token_details = activity_analyses.get(token_mint, {}).get(\"token_details\", {})\n",
    "    token_holders = activity_analyses.get(token_mint, {}).get(\"holders\", [])\n",
    "    \n",
    "    # Convert transfers to DataFrame if available\n",
    "    transfers = activity_analyses.get(token_mint, {}).get(\"transfers\", [])\n",
    "    token_transfers = pd.DataFrame(transfers) if transfers else pd.DataFrame()\n",
    "    \n",
    "    # Get insider data\n",
    "    token_insider_data = insider_analyses.get(token_mint, {})\n",
    "    \n",
    "    # Get RugCheck risk data\n",
    "    rugcheck_risk = pd.DataFrame([token]) if \"risks\" in token else pd.DataFrame()\n",
    "    \n",
    "    # Calculate risk score\n",
    "    risk_score = calculate_token_risk(\n",
    "        token_mint,\n",
    "        token_details,\n",
    "        token_holders,\n",
    "        token_transfers,\n",
    "        token_insider_data,\n",
    "        rugcheck_risk\n",
    "    )\n",
    "    \n",
    "    token_risk_scores[token_mint] = risk_score\n",
    "    \n",
    "    print(f\"\\nRisk assessment for {token['symbol']} ({token_mint[:10]}...):\")\n",
    "    print(f\"- Overall score: {risk_score['risk_score']:.2f}\")\n",
    "    print(f\"- Risk level: {risk_score['risk_level']}\")\n",
    "    print(\"- Top risk factors:\")\n",
    "    for factor in risk_score['risk_factors'][:3]:\n",
    "        print(f\"  * {factor['name']}: {factor['description']} (score: {factor['score']})\")\n",
    "    \n",
    "    # Generate risk visualization\n",
    "    visualize_risk_score(risk_score, output_file=f\"../../data/visualizations/risk_score_{token_mint}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9790f",
   "metadata": {},
   "source": [
    "## Rug Pull Likelihood Assessment\n",
    "\n",
    "Let's assess the likelihood of a rug pull for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assess rug pull likelihood\n",
    "def assess_rug_pull_likelihood(token_mint):\n",
    "    # Get relevant analyses\n",
    "    creator_data = creator_analyses.get(token_mint, {})\n",
    "    insider_data = insider_analyses.get(token_mint, {})\n",
    "    activity_data = activity_analyses.get(token_mint, {})\n",
    "    risk_score = token_risk_scores.get(token_mint, {})\n",
    "    \n",
    "    # Initialize scores\n",
    "    team_credibility_score = 0\n",
    "    liquidity_risk_score = 0\n",
    "    token_structure_score = 0\n",
    "    trading_pattern_score = 0\n",
    "    \n",
    "    # Calculate team credibility score (0-100, higher is worse)\n",
    "    if creator_data:\n",
    "        # Check if creator has history of rug pulls\n",
    "        if creator_data.get(\"has_rug_history\"):\n",
    "            team_credibility_score += 80\n",
    "        \n",
    "        # Check creator's average token risk score\n",
    "        avg_risk = creator_data.get(\"avg_risk_score\", 0)\n",
    "        team_credibility_score += min(40, avg_risk * 0.5)\n",
    "        \n",
    "        # Multiple tokens is slightly suspicious\n",
    "        token_count = creator_data.get(\"creator_tokens_count\", 0)\n",
    "        if token_count > 10:\n",
    "            team_credibility_score += 30\n",
    "        elif token_count > 5:\n",
    "            team_credibility_score += 20\n",
    "        elif token_count > 2:\n",
    "            team_credibility_score += 10\n",
    "    \n",
    "    # Calculate liquidity risk score\n",
    "    if insider_data:\n",
    "        # Check liquidity locked percentage\n",
    "        locked_pct = insider_data.get(\"liquidity_locked_pct\", 0)\n",
    "        if locked_pct < 50:\n",
    "            liquidity_risk_score += 80  # Very high risk if <50% locked\n",
    "        elif locked_pct < 80:\n",
    "            liquidity_risk_score += 40  # Medium risk if <80% locked\n",
    "        \n",
    "        # Check total liquidity\n",
    "        total_liquidity = insider_data.get(\"total_liquidity\", 0)\n",
    "        if total_liquidity < 1000:\n",
    "            liquidity_risk_score += 80  # Very high risk if <$1k\n",
    "        elif total_liquidity < 10000:\n",
    "            liquidity_risk_score += 40  # Medium risk if <$10k\n",
    "    \n",
    "    # Calculate token structure score\n",
    "    token_report = None\n",
    "    try:\n",
    "        token_report = rugcheck.get_token_report(token_mint)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if token_report:\n",
    "        # Check mint authority\n",
    "        if token_report.get(\"mintAuthority\"):\n",
    "            token_structure_score += 50  # High risk if mint authority exists\n",
    "        \n",
    "        # Check freeze authority\n",
    "        if token_report.get(\"freezeAuthority\"):\n",
    "            token_structure_score += 30  # Medium risk if freeze authority exists\n",
    "        \n",
    "        # Check token program\n",
    "        token_program = token_report.get(\"tokenProgram\")\n",
    "        if token_program != \"TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA\":  # Not standard SPL\n",
    "            token_structure_score += 30  # Non-standard token program is risky\n",
    "    \n",
    "    # Calculate trading pattern score\n",
    "    if activity_data:\n",
    "        # Check wash trading indicators\n",
    "        wash_indicators = activity_data.get(\"wash_trading_indicators\", 0)\n",
    "        if wash_indicators > 5:\n",
    "            trading_pattern_score += 80  # Very high risk if many wash indicators\n",
    "        elif wash_indicators > 2:\n",
    "            trading_pattern_score += 40  # Medium risk if some wash indicators\n",
    "    \n",
    "    # Calculate overall rug pull likelihood\n",
    "    # Weighted average of component scores\n",
    "    overall_score = (\n",
    "        team_credibility_score * 0.3 +\n",
    "        liquidity_risk_score * 0.4 +\n",
    "        token_structure_score * 0.2 +\n",
    "        trading_pattern_score * 0.1\n",
    "    )\n",
    "    \n",
    "    # Determine likelihood category\n",
    "    if overall_score >= 80:\n",
    "        likelihood = \"Very High\"\n",
    "    elif overall_score >= 60:\n",
    "        likelihood = \"High\"\n",
    "    elif overall_score >= 40:\n",
    "        likelihood = \"Medium\"\n",
    "    elif overall_score >= 20:\n",
    "        likelihood = \"Low\"\n",
    "    else:\n",
    "        likelihood = \"Very Low\"\n",
    "    \n",
    "    return {\n",
    "        \"token_mint\": token_mint,\n",
    "        \"rug_pull_likelihood\": likelihood,\n",
    "        \"overall_score\": overall_score,\n",
    "        \"team_credibility_score\": team_credibility_score,\n",
    "        \"liquidity_risk_score\": liquidity_risk_score,\n",
    "        \"token_structure_score\": token_structure_score,\n",
    "        \"trading_pattern_score\": trading_pattern_score,\n",
    "        \"reasons\": {\n",
    "            \"team_credibility\": [\n",
    "                \"Creator has history of rug pulls\" if creator_data.get(\"has_rug_history\") else None,\n",
    "                f\"Creator has launched multiple tokens ({creator_data.get('creator_tokens_count', 0)})\" if creator_data.get(\"creator_tokens_count\", 0) > 1 else None,\n",
    "                f\"Creator's tokens have high average risk ({creator_data.get('avg_risk_score', 0):.2f})\" if creator_data.get(\"avg_risk_score\", 0) > 50 else None\n",
    "            ],\n",
    "            \"liquidity_risk\": [\n",
    "                f\"Low liquidity lock percentage ({insider_data.get('liquidity_locked_pct', 0):.2f}%)\" if insider_data.get(\"liquidity_locked_pct\", 100) < 80 else None,\n",
    "                f\"Low total liquidity (${insider_data.get('total_liquidity', 0):.2f})\" if insider_data.get(\"total_liquidity\", 100000) < 10000 else None\n",
    "            ],\n",
    "            \"token_structure\": [\n",
    "                \"Mint authority is active\" if token_report and token_report.get(\"mintAuthority\") else None,\n",
    "                \"Freeze authority is active\" if token_report and token_report.get(\"freezeAuthority\") else None,\n",
    "                \"Non-standard token program\" if token_report and token_report.get(\"tokenProgram\") != \"TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA\" else None\n",
    "            ],\n",
    "            \"trading_pattern\": [\n",
    "                f\"Wash trading indicators detected ({activity_data.get('wash_trading_indicators', 0)})\" if activity_data.get(\"wash_trading_indicators\", 0) > 0 else None\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Assess rug pull likelihood for each token\n",
    "rug_pull_assessments = {}\n",
    "for token in suspicious_tokens:\n",
    "    token_mint = token[\"mint\"]\n",
    "    assessment = assess_rug_pull_likelihood(token_mint)\n",
    "    rug_pull_assessments[token_mint] = assessment\n",
    "    \n",
    "    print(f\"\\nRug pull assessment for {token['symbol']} ({token_mint[:10]}...):\")\n",
    "    print(f\"- Likelihood: {assessment['rug_pull_likelihood']} ({assessment['overall_score']:.2f}/100)\")\n",
    "    print(\"- Key factors:\")\n",
    "    \n",
    "    for category, reasons in assessment[\"reasons\"].items():\n",
    "        valid_reasons = [r for r in reasons if r]\n",
    "        if valid_reasons:\n",
    "            print(f\"  * {category.replace('_', ' ').title()}:\")\n",
    "            for reason in valid_reasons:\n",
    "                print(f\"    - {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25048f4a",
   "metadata": {},
   "source": [
    "## Conclusions and Report Generation\n",
    "\n",
    "Let's summarize our findings and generate a report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = [\"# Token Insider Analysis Report\\n\"]\n",
    "report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "report.append(\"## Tokens Analyzed\\n\")\n",
    "for token in suspicious_tokens:\n",
    "    token_mint = token[\"mint\"]\n",
    "    risk_level = token_risk_scores[token_mint]['risk_level'] if token_mint in token_risk_scores else \"Unknown\"\n",
    "    rug_likelihood = rug_pull_assessments[token_mint]['rug_pull_likelihood'] if token_mint in rug_pull_assessments else \"Unknown\"\n",
    "    report.append(f\"- **{token['symbol']}** ({token_mint[:10]}...): Risk Level: {risk_level}, Rug Pull Likelihood: {rug_likelihood}\")\n",
    "\n",
    "for token in suspicious_tokens:\n",
    "    token_mint = token[\"mint\"]\n",
    "    token_symbol = token[\"symbol\"]\n",
    "    token_name = token.get(\"name\", \"Unknown\")\n",
    "    \n",
    "    report.append(f\"\\n## Analysis for {token_symbol} ({token_name})\\n\")\n",
    "    report.append(f\"Mint Address: `{token_mint}`\\n\")\n",
    "    \n",
    "    # Risk assessment\n",
    "    if token_mint in token_risk_scores:\n",
    "        report.append(f\"### Risk Assessment\\n\")\n",
    "        report.append(f\"- Overall risk score: {token_risk_scores[token_mint]['risk_score']:.2f}\")\n",
    "        report.append(f\"- Risk level: {token_risk_scores[token_mint]['risk_level']}\\n\")\n",
    "        \n",
    "        report.append(\"#### Risk Factors\\n\")\n",
    "        for factor in token_risk_scores[token_mint]['risk_factors']:\n",
    "            report.append(f\"- **{factor['name']}**: {factor['description']} (score: {factor['score']})\")\n",
    "        \n",
    "        report.append(f\"\\n![Risk Score](../../data/visualizations/risk_score_{token_mint}.png)\")\n",
    "    \n",
    "    # Creator analysis\n",
    "    if token_mint in creator_analyses and creator_analyses[token_mint]:\n",
    "        creator_data = creator_analyses[token_mint]\n",
    "        creator_address = creator_data.get(\"creator_address\")\n",
    "        \n",
    "        if creator_address:\n",
    "            report.append(f\"\\n### Creator Analysis\\n\")\n",
    "            report.append(f\"- Creator address: `{creator_address}`\")\n",
    "            report.append(f\"- Total tokens created: {creator_data.get('creator_tokens_count', 0)}\")\n",
    "            report.append(f\"- Average risk of other tokens: {creator_data.get('avg_risk_score', 0):.2f}\")\n",
    "            report.append(f\"- History of rug pulls: {'Yes' if creator_data.get('has_rug_history') else 'No'}\\n\")\n",
    "            \n",
    "            other_tokens = creator_data.get(\"other_tokens_risk\", [])\n",
    "            if other_tokens:\n",
    "                report.append(\"#### Other Tokens from Same Creator\\n\")\n",
    "                for other_token in other_tokens[:5]:  # Show top 5\n",
    "                    report.append(f\"- `{other_token['mint'][:10]}...`: Risk score {other_token['risk_score']:.2f}\")\n",
    "    \n",
    "    # Insider analysis\n",
    "    if token_mint in insider_analyses and insider_analyses[token_mint]:\n",
    "        insider_data = insider_analyses[token_mint]\n",
    "        \n",
    "        report.append(f\"\\n### Insider Network Analysis\\n\")\n",
    "        report.append(f\"- Network size: {insider_data.get('node_count', 0)} addresses, {insider_data.get('edge_count', 0)} connections\")\n",
    "        report.append(f\"- Communities detected: {len(insider_data.get('communities', {}))}\")\n",
    "        report.append(f\"- Top holder concentration: {insider_data.get('top_holder_pct', 0):.2f}%\")\n",
    "        report.append(f\"- Top 5 holders concentration: {insider_data.get('top5_pct', 0):.2f}%\")\n",
    "        report.append(f\"- Insider wallets control: {insider_data.get('insider_pct', 0):.2f}% of supply\")\n",
    "        report.append(f\"- Total liquidity: ${insider_data.get('total_liquidity', 0):.2f}\")\n",
    "        report.append(f\"- Liquidity locked: {insider_data.get('liquidity_locked_pct', 0):.2f}%\\n\")\n",
    "        \n",
    "        suspicious_patterns = insider_data.get(\"suspicious_patterns\", [])\n",
    "        if suspicious_patterns:\n",
    "            report.append(\"#### Suspicious Patterns Detected\\n\")\n",
    "            for pattern in suspicious_patterns:\n",
    "                report.append(f\"- **{pattern['type']}**: {pattern['description']} (risk: {pattern['risk_score']})\")\n",
    "    \n",
    "    # Token activity analysis\n",
    "    if token_mint in activity_analyses and activity_analyses[token_mint]:\n",
    "        activity_data = activity_analyses[token_mint]\n",
    "        \n",
    "        report.append(f\"\\n### Token Activity Analysis\\n\")\n",
    "        report.append(f\"- Wash trading indicators: {activity_data.get('wash_trading_indicators', 0)}\\n\")\n",
    "        \n",
    "        wash_reasons = activity_data.get(\"wash_trading_reasons\", [])\n",
    "        if wash_reasons:\n",
    "            report.append(\"#### Potential Wash Trading Evidence\\n\")\n",
    "            for reason in wash_reasons:\n",
    "                report.append(f\"- {reason}\")\n",
    "        \n",
    "        report.append(f\"\\n![Token Activity](../../data/visualizations/token_activity_{token_mint}.png)\")\n",
    "    \n",
    "    # Rug pull assessment\n",
    "    if token_mint in rug_pull_assessments:\n",
    "        assessment = rug_pull_assessments[token_mint]\n",
    "        \n",
    "        report.append(f\"\\n### Rug Pull Assessment\\n\")\n",
    "        report.append(f\"- **Likelihood**: {assessment['rug_pull_likelihood']} ({assessment['overall_score']:.2f}/100)\")\n",
    "        report.append(f\"- Team credibility score: {assessment['team_credibility_score']:.2f}/100\")\n",
    "        report.append(f\"- Liquidity risk score: {assessment['liquidity_risk_score']:.2f}/100\")\n",
    "        report.append(f\"- Token structure score: {assessment['token_structure_score']:.2f}/100\")\n",
    "        report.append(f\"- Trading pattern score: {assessment['trading_pattern_score']:.2f}/100\\n\")\n",
    "        \n",
    "        report.append(\"#### Key Risk Factors\\n\")\n",
    "        for category, reasons in assessment[\"reasons\"].items():\n",
    "            valid_reasons = [r for r in reasons if r]\n",
    "            if valid_reasons:\n",
    "                report.append(f\"**{category.replace('_', ' ').title()}**:\")\n",
    "                for reason in valid_reasons:\n",
    "                    report.append(f\"- {reason}\")\n",
    "                report.append(\"\")\n",
    "\n",
    "# Save report to file\n",
    "report_path = f\"../../reports/token_insider_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "os.makedirs(\"../../reports\", exist_ok=True)\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(report))\n",
    "\n",
    "print(f\"\\nReport generated and saved to {report_path}\")\n",
    "\n",
    "# Display report in notebook\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(\"\\n\".join(report)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
