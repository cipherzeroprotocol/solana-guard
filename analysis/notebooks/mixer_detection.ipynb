{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab74bcd0",
   "metadata": {},
   "source": [
    "# Cryptocurrency Mixer Detection Notebook\n",
    "\n",
    "This notebook analyzes transaction patterns to detect the usage of cryptocurrency mixers on the Solana blockchain.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Cryptocurrency mixers (or tumblers) are services that obfuscate transaction trails by mixing multiple users' funds together. This notebook:\n",
    "\n",
    "1. Identifies known mixer services on Solana\n",
    "2. Detects transactions with mixer-like patterns\n",
    "3. Analyzes on-chain transaction patterns indicative of mixer usage\n",
    "4. Traces funds that have passed through mixers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39c3d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils modules imported from data_collection.utils\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import SolanaGuard modules\n",
    "from data_collection.collectors.helius_collector import HeliusCollector\n",
    "from data_collection.collectors.range_collector import RangeCollector\n",
    "\n",
    "# Try to locate the utils modules (could be in different locations)\n",
    "try:\n",
    "    # First try the most likely location - as a top-level package\n",
    "    from utils.entropy_analysis import calculate_transaction_entropy, detect_entropy_anomalies\n",
    "    from utils.graph_utils import TransactionFlowGraph\n",
    "    from utils.risk_scoring import calculate_address_risk\n",
    "    from utils.visualization import visualize_transaction_flow, plot_entropy_distribution\n",
    "    print(\"Utils modules imported successfully\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try as a subpackage of data_collection\n",
    "        from data_collection.utils.entropy_analysis import calculate_transaction_entropy, detect_entropy_anomalies\n",
    "        from data_collection.utils.graph_utils import TransactionFlowGraph\n",
    "        from data_collection.utils.risk_scoring import calculate_address_risk\n",
    "        from data_collection.utils.visualization import visualize_transaction_flow, plot_entropy_distribution\n",
    "        print(\"Utils modules imported from data_collection.utils\")\n",
    "    except ImportError:\n",
    "        # Define simple placeholder functions if modules can't be found\n",
    "        print(\"WARNING: Could not import utils modules. Using placeholder functions.\")\n",
    "        \n",
    "        def calculate_transaction_entropy(data):\n",
    "            return 0.0\n",
    "            \n",
    "        def detect_entropy_anomalies(data):\n",
    "            return []\n",
    "            \n",
    "        class TransactionFlowGraph:\n",
    "            def __init__(self):\n",
    "                self.graph = nx.DiGraph()\n",
    "                \n",
    "            def add_transaction(self, source, target, **kwargs):\n",
    "                if source and target:\n",
    "                    self.graph.add_edge(source, target)\n",
    "                    \n",
    "            def get_nodes(self):\n",
    "                return list(self.graph.nodes())\n",
    "                \n",
    "            def get_edge_count(self):\n",
    "                return len(self.graph.edges())\n",
    "                \n",
    "            def set_node_attribute(self, node, attr, value):\n",
    "                self.graph.nodes[node][attr] = value\n",
    "                \n",
    "            def get_node_attribute(self, node, attr):\n",
    "                return self.graph.nodes[node].get(attr, False)\n",
    "                \n",
    "            # Add other required methods with simple implementations\n",
    "            def calculate_centrality(self):\n",
    "                return {node: 0.0 for node in self.graph.nodes()}\n",
    "                \n",
    "            def identify_communities(self):\n",
    "                return [[node] for node in self.graph.nodes()]\n",
    "                \n",
    "            # Add more methods as needed\n",
    "            def get_in_neighbors(self, node):\n",
    "                return list(self.graph.predecessors(node)) if node in self.graph else []\n",
    "                \n",
    "            def get_out_neighbors(self, node):\n",
    "                return list(self.graph.successors(node)) if node in self.graph else []\n",
    "                \n",
    "            def get_edge_attributes(self, source, target):\n",
    "                return {\"transactions\": []}\n",
    "                \n",
    "            def export_to_json(self):\n",
    "                return {\"nodes\": [], \"edges\": []}\n",
    "        \n",
    "        def calculate_address_risk(address):\n",
    "            return 0.0\n",
    "            \n",
    "        def visualize_transaction_flow(graph, highlight_nodes=None, output_file=None):\n",
    "            return output_file if output_file else None\n",
    "            \n",
    "        def plot_entropy_distribution(data):\n",
    "            pass\n",
    "\n",
    "# Configure plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61349ec",
   "metadata": {},
   "source": [
    "## Initialize API Collectors\n",
    "\n",
    "First, we initialize the necessary API collectors to gather blockchain data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66a29f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:34:06,711 - helius_collector - INFO - Initialized Helius collector\n",
      "2025-04-24 22:34:06,712 - range_collector - INFO - Initialized Range collector\n",
      "2025-04-24 22:34:06,712 - range_collector - INFO - Initialized Range collector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error connecting to APIs: 'HeliusCollector' object has no attribute 'check_connection'\n"
     ]
    }
   ],
   "source": [
    "# Initialize collectors\n",
    "helius = HeliusCollector()\n",
    "range_api = RangeCollector()\n",
    "\n",
    "# Verify API connectivity\n",
    "try:\n",
    "    helius_status = helius.check_connection()\n",
    "    range_status = range_api.check_connection()\n",
    "    print(f\"Helius API: {'Connected' if helius_status else 'Connection failed'}\")\n",
    "    print(f\"Range API: {'Connected' if range_status else 'Connection failed'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to APIs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65366c26",
   "metadata": {},
   "source": [
    "## Known Mixer Services and Addresses\n",
    "\n",
    "Let's define a database of known mixer services and their associated addresses on Solana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b784ba5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 known mixer addresses across 3 mixer services\n",
      "- tornado_cash_solana (high risk): 2 addresses\n",
      "- elusiv (medium risk): 2 addresses\n",
      "- cyclos_mixer (medium risk): 2 addresses\n"
     ]
    }
   ],
   "source": [
    "# Database of known mixers on Solana\n",
    "KNOWN_MIXERS = {\n",
    "    \"tornado_cash_solana\": {\n",
    "        \"addresses\": [\n",
    "            \"TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8\", # Main contract\n",
    "            \"TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z\"  # Router\n",
    "        ],\n",
    "        \"transaction_patterns\": [\"equal_amounts\", \"fixed_intervals\", \"privacy_set\"],\n",
    "        \"risk_level\": \"high\"\n",
    "    },\n",
    "    \"elusiv\": {\n",
    "        \"addresses\": [\n",
    "            \"E1w8SZpBPkRBdBmEJUwpRZx1SbQVVhNqkuH95uKJvypH\",  # Program\n",
    "            \"2EgZ5LuMqyVKQYS4AFhJRpZNr1rLxuU4UJuNJbcKFubu\"   # Fee collector\n",
    "        ],\n",
    "        \"transaction_patterns\": [\"stealth_addresses\", \"ring_signature\", \"decoy_outputs\"],\n",
    "        \"risk_level\": \"medium\"\n",
    "    },\n",
    "    \"cyclos_mixer\": {\n",
    "        \"addresses\": [\n",
    "            \"CYcLEsDHNZn8mVimVJLMFeYRGzdPx9QmxUL5kgKSTsdq\",  # Mixer contract\n",
    "            \"CYCSaAMM4tJLXeXQKVAZrkLUKu8gYXhRNfYJG5qkKPQt\"   # Pool\n",
    "        ],\n",
    "        \"transaction_patterns\": [\"pool_deposits\", \"time_locks\", \"uniform_withdrawals\"],\n",
    "        \"risk_level\": \"medium\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to get all mixer addresses\n",
    "def get_all_mixer_addresses() -> List[str]:\n",
    "    all_addresses = []\n",
    "    for mixer_name, mixer_info in KNOWN_MIXERS.items():\n",
    "        all_addresses.extend(mixer_info[\"addresses\"])\n",
    "    return all_addresses\n",
    "\n",
    "# Function to check if address is a known mixer\n",
    "def is_known_mixer(address: str) -> Tuple[bool, str]:\n",
    "    for mixer_name, mixer_info in KNOWN_MIXERS.items():\n",
    "        if address in mixer_info[\"addresses\"]:\n",
    "            return True, mixer_name\n",
    "    return False, \"\"\n",
    "\n",
    "# List all known mixer addresses\n",
    "all_mixer_addresses = get_all_mixer_addresses()\n",
    "print(f\"Loaded {len(all_mixer_addresses)} known mixer addresses across {len(KNOWN_MIXERS)} mixer services\")\n",
    "for mixer_name, mixer_info in KNOWN_MIXERS.items():\n",
    "    print(f\"- {mixer_name} ({mixer_info['risk_level']} risk): {len(mixer_info['addresses'])} addresses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98f152",
   "metadata": {},
   "source": [
    "## Analyze Known Mixer Transactions\n",
    "\n",
    "Let's analyze transactions involving the known mixer addresses to understand their patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96d22e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cluster transactions by pattern\n",
    "def cluster_transactions_by_pattern(tx_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Cluster transactions to identify common patterns associated with mixer services.\n",
    "    \n",
    "    Args:\n",
    "        tx_df: DataFrame containing transaction data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of identified patterns with descriptions and counts\n",
    "    \"\"\"\n",
    "    patterns = {}\n",
    "    pattern_id = 1\n",
    "    \n",
    "    # Check if DataFrame has required columns\n",
    "    if tx_df.empty:\n",
    "        return patterns\n",
    "    \n",
    "    # 1. Look for equal amount patterns (common in mixers)\n",
    "    if 'amount' in tx_df.columns:\n",
    "        # Group by amount and count\n",
    "        amount_counts = tx_df['amount'].value_counts()\n",
    "        # Find amounts that appear multiple times (indicating possible mixer patterns)\n",
    "        common_amounts = amount_counts[amount_counts > 3]\n",
    "        \n",
    "        if not common_amounts.empty:\n",
    "            for amount, count in common_amounts.items():\n",
    "                if count >= 5:  # At least 5 transactions with the same amount\n",
    "                    patterns[f\"pattern_{pattern_id}\"] = {\n",
    "                        \"description\": f\"Fixed amount transfers of {amount:.4f} units\",\n",
    "                        \"count\": int(count),\n",
    "                        \"type\": \"equal_amounts\",\n",
    "                        \"confidence\": min(0.9, 0.5 + count/50)  # Higher confidence with more occurrences\n",
    "                    }\n",
    "                    pattern_id += 1\n",
    "    \n",
    "    # 2. Look for time interval patterns\n",
    "    if 'block_time' in tx_df.columns:\n",
    "        # Sort by timestamp\n",
    "        sorted_tx = tx_df.sort_values('block_time')\n",
    "        # Calculate time differences\n",
    "        time_diffs = sorted_tx['block_time'].diff().dropna()\n",
    "        \n",
    "        if not time_diffs.empty:\n",
    "            # Group by time difference (rounded to nearest 10 seconds)\n",
    "            rounded_diffs = (time_diffs / 10).round() * 10\n",
    "            diff_counts = rounded_diffs.value_counts()\n",
    "            common_diffs = diff_counts[diff_counts > 3]\n",
    "            \n",
    "            if not common_diffs.empty:\n",
    "                for diff_seconds, count in common_diffs.items():\n",
    "                    if count >= 4:  # At least 4 transactions with similar timing\n",
    "                        time_desc = f\"{diff_seconds:.0f} seconds\" if diff_seconds < 300 else f\"{diff_seconds/60:.1f} minutes\"\n",
    "                        patterns[f\"pattern_{pattern_id}\"] = {\n",
    "                            \"description\": f\"Regular time intervals of approximately {time_desc}\",\n",
    "                            \"count\": int(count),\n",
    "                            \"type\": \"fixed_intervals\",\n",
    "                            \"confidence\": min(0.85, 0.4 + count/40)\n",
    "                        }\n",
    "                        pattern_id += 1\n",
    "    \n",
    "    # 3. Look for mixing patterns (many inputs, many outputs)\n",
    "    if 'signature' in tx_df.columns and ('sender_address' in tx_df.columns or 'receiver_address' in tx_df.columns):\n",
    "        # Count unique senders and receivers\n",
    "        sender_col = 'sender_address' if 'sender_address' in tx_df.columns else 'source_address'\n",
    "        receiver_col = 'receiver_address' if 'receiver_address' in tx_df.columns else 'target_address'\n",
    "        \n",
    "        if sender_col in tx_df.columns and receiver_col in tx_df.columns:\n",
    "            unique_senders = tx_df[sender_col].nunique()\n",
    "            unique_receivers = tx_df[receiver_col].nunique()\n",
    "            \n",
    "            # High mixing pattern - many senders to many receivers\n",
    "            if unique_senders > 10 and unique_receivers > 10:\n",
    "                patterns[f\"pattern_{pattern_id}\"] = {\n",
    "                    \"description\": f\"High mixing pattern with {unique_senders} senders and {unique_receivers} receivers\",\n",
    "                    \"count\": len(tx_df),\n",
    "                    \"type\": \"privacy_set\",\n",
    "                    \"confidence\": min(0.95, 0.6 + (unique_senders + unique_receivers)/100)\n",
    "                }\n",
    "                pattern_id += 1\n",
    "    \n",
    "    # 4. Look for fragmentation patterns (one input, many similar outputs)\n",
    "    if sender_col in tx_df.columns and receiver_col in tx_df.columns and 'amount' in tx_df.columns:\n",
    "        # Group by sender\n",
    "        for sender, group in tx_df.groupby(sender_col):\n",
    "            if len(group) >= 5:  # Sender with multiple transactions\n",
    "                amount_std = group['amount'].std()\n",
    "                amount_mean = group['amount'].mean()\n",
    "                if amount_mean > 0 and (amount_std / amount_mean) < 0.2:  # Low variance in amounts\n",
    "                    patterns[f\"pattern_{pattern_id}\"] = {\n",
    "                        \"description\": f\"Fragmentation pattern from {sender} with {len(group)} similar outputs\",\n",
    "                        \"count\": len(group),\n",
    "                        \"type\": \"fragmentation\",\n",
    "                        \"confidence\": min(0.9, 0.5 + len(group)/20)\n",
    "                    }\n",
    "                    pattern_id += 1\n",
    "    \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cf9b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:34:06,768 - helius_collector - INFO - Fetching transaction history for TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8 (limit: 100)\n",
      "2025-04-24 22:34:06,769 - helius_collector - INFO - Getting signatures for TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8 (limit: 100)\n",
      "2025-04-24 22:34:06,769 - helius_collector - INFO - Getting signatures for TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8 (limit: 100)\n",
      "2025-04-24 22:34:06,826 - helius_collector - ERROR - Failed to make RPC request: 401 Client Error: Unauthorized for url: https://mainnet.helius-rpc.com/?api-key=None\n",
      "2025-04-24 22:34:06,827 - helius_collector - INFO - Fetching transaction history for TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z (limit: 100)\n",
      "2025-04-24 22:34:06,828 - helius_collector - INFO - Getting signatures for TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z (limit: 100)\n",
      "2025-04-24 22:34:06,826 - helius_collector - ERROR - Failed to make RPC request: 401 Client Error: Unauthorized for url: https://mainnet.helius-rpc.com/?api-key=None\n",
      "2025-04-24 22:34:06,827 - helius_collector - INFO - Fetching transaction history for TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z (limit: 100)\n",
      "2025-04-24 22:34:06,828 - helius_collector - INFO - Getting signatures for TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z (limit: 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing mixer address: TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8\n",
      "Error analyzing mixer address TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8: Request failed: 401 Client Error: Unauthorized for url: https://mainnet.helius-rpc.com/?api-key=None\n",
      "Analyzing mixer address: TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:34:07,017 - helius_collector - ERROR - Failed to make RPC request: 401 Client Error: Unauthorized for url: https://mainnet.helius-rpc.com/?api-key=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing mixer address TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z: Request failed: 401 Client Error: Unauthorized for url: https://mainnet.helius-rpc.com/?api-key=None\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze mixer transactions\n",
    "def analyze_mixer_transactions(mixer_addresses: List[str], max_txs_per_address: int = 100) -> Dict:\n",
    "    mixer_tx_data = {\n",
    "        \"transactions\": [],\n",
    "        \"address_stats\": {},\n",
    "        \"common_patterns\": {}\n",
    "    }\n",
    "    \n",
    "    for address in mixer_addresses:\n",
    "        print(f\"Analyzing mixer address: {address}\")\n",
    "        is_mixer, mixer_name = is_known_mixer(address)\n",
    "        \n",
    "        if not is_mixer:\n",
    "            print(f\"Warning: {address} is not in our known mixer database\")\n",
    "            mixer_name = \"unknown_mixer\"\n",
    "        \n",
    "        # Get transaction history for this mixer address\n",
    "        try:\n",
    "            tx_history = helius.fetch_transaction_history(address, limit=max_txs_per_address)\n",
    "            print(f\"Fetched {len(tx_history)} transactions\")\n",
    "            \n",
    "            # Add mixer info to each transaction\n",
    "            for tx in tx_history:\n",
    "                tx[\"mixer_name\"] = mixer_name\n",
    "                tx[\"mixer_address\"] = address\n",
    "                mixer_tx_data[\"transactions\"].append(tx)\n",
    "            \n",
    "            # Analyze token transfers for this address\n",
    "            token_transfers = helius.analyze_token_transfers(address, limit=max_txs_per_address)\n",
    "            print(f\"Analyzed {len(token_transfers)} token transfers\")\n",
    "            \n",
    "            # Calculate statistics for this address\n",
    "            if len(tx_history) > 0:\n",
    "                # Extract timestamps and sort them\n",
    "                timestamps = sorted([tx.get(\"block_time\", 0) for tx in tx_history if tx.get(\"block_time\")])\n",
    "                \n",
    "                # Calculate time differences between transactions\n",
    "                time_diffs = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "                median_time_diff = np.median(time_diffs) if time_diffs else 0\n",
    "                std_time_diff = np.std(time_diffs) if len(time_diffs) > 1 else 0\n",
    "                \n",
    "                # Calculate transaction amount statistics\n",
    "                amounts = [tx.get(\"amount\", 0) for tx in token_transfers if tx.get(\"amount\")]\n",
    "                median_amount = np.median(amounts) if amounts else 0\n",
    "                unique_amounts = len(set(amounts)) if amounts else 0\n",
    "                amount_uniformity = 1 - (unique_amounts / len(amounts)) if amounts else 0\n",
    "                \n",
    "                # Store statistics\n",
    "                mixer_tx_data[\"address_stats\"][address] = {\n",
    "                    \"tx_count\": len(tx_history),\n",
    "                    \"transfer_count\": len(token_transfers),\n",
    "                    \"median_time_diff\": median_time_diff,\n",
    "                    \"time_diff_std\": std_time_diff,\n",
    "                    \"median_amount\": median_amount,\n",
    "                    \"unique_amounts\": unique_amounts,\n",
    "                    \"amount_uniformity\": amount_uniformity,  # Higher = more uniform amounts\n",
    "                    \"first_seen\": min(timestamps) if timestamps else 0,\n",
    "                    \"last_seen\": max(timestamps) if timestamps else 0\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing mixer address {address}: {e}\")\n",
    "    \n",
    "    # Identify common patterns across all mixer transactions\n",
    "    if mixer_tx_data[\"transactions\"]:\n",
    "        # Convert to DataFrame for easier analysis\n",
    "        tx_df = pd.DataFrame(mixer_tx_data[\"transactions\"])\n",
    "        \n",
    "        # Apply clustering to identify transaction patterns\n",
    "        if len(tx_df) > 10:  # Need sufficient data for clustering\n",
    "            patterns = cluster_transactions_by_pattern(tx_df)\n",
    "            mixer_tx_data[\"common_patterns\"] = patterns\n",
    "            \n",
    "            print(f\"\\nIdentified {len(patterns)} common transaction patterns across mixers:\")\n",
    "            for pattern_id, pattern_info in patterns.items():\n",
    "                print(f\"- Pattern {pattern_id}: {pattern_info['description']} \"\n",
    "                      f\"(found in {pattern_info['count']} transactions)\")\n",
    "    \n",
    "    return mixer_tx_data\n",
    "\n",
    "# Analyze a sample of mixer addresses (limit to 2 for demonstration)\n",
    "sample_mixer_addresses = all_mixer_addresses[:2]\n",
    "mixer_transaction_data = analyze_mixer_transactions(sample_mixer_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d78967",
   "metadata": {},
   "source": [
    "## Analyze Transaction Entropy for Mixer Detection\n",
    "\n",
    "Transaction entropy analysis helps detect the randomization patterns typical of mixer services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1d3ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No transactions available for entropy analysis\n"
     ]
    }
   ],
   "source": [
    "# Calculate entropy scores for mixer transactions\n",
    "def calculate_mixer_entropy_scores(tx_data: Dict) -> Dict:\n",
    "    transactions = tx_data.get(\"transactions\", [])\n",
    "    if not transactions:\n",
    "        print(\"No transactions available for entropy analysis\")\n",
    "        return {\"entropy_scores\": [], \"anomalies\": []}\n",
    "    \n",
    "    # Convert to DataFrame if it's not already\n",
    "    if not isinstance(transactions, pd.DataFrame):\n",
    "        tx_df = pd.DataFrame(transactions)\n",
    "    else:\n",
    "        tx_df = transactions\n",
    "        \n",
    "    print(f\"Calculating entropy scores for {len(tx_df)} transactions...\")\n",
    "    \n",
    "    # Create features for entropy calculation\n",
    "    features = []\n",
    "    \n",
    "    # Token transfer amounts (if available)\n",
    "    if \"amount\" in tx_df.columns:\n",
    "        features.append(\"amount\")\n",
    "    \n",
    "    # Transaction timestamps (if available)\n",
    "    if \"block_time\" in tx_df.columns:\n",
    "        # Convert to pd.Timestamp objects for better analysis\n",
    "        tx_df[\"timestamp\"] = pd.to_datetime(tx_df[\"block_time\"], unit=\"s\")\n",
    "        # Extract hour of day as a cyclical feature\n",
    "        tx_df[\"hour\"] = tx_df[\"timestamp\"].dt.hour\n",
    "        features.append(\"hour\")\n",
    "        \n",
    "        # Calculate time differences between consecutive transactions\n",
    "        tx_df = tx_df.sort_values(\"block_time\")\n",
    "        tx_df[\"time_diff\"] = tx_df[\"block_time\"].diff()\n",
    "        features.append(\"time_diff\")\n",
    "    \n",
    "    # Use entropy analysis utility to calculate entropy of transaction patterns\n",
    "    entropy_scores = []\n",
    "    for feature in features:\n",
    "        if feature in tx_df.columns and not tx_df[feature].isnull().all():\n",
    "            try:\n",
    "                entropy = calculate_transaction_entropy(tx_df[feature].dropna().values)\n",
    "                entropy_scores.append({\n",
    "                    \"feature\": feature,\n",
    "                    \"entropy\": entropy,\n",
    "                    \"normalized_entropy\": entropy / np.log2(len(tx_df[feature].dropna().unique()))\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating entropy for {feature}: {e}\")\n",
    "    \n",
    "    print(f\"Calculated {len(entropy_scores)} entropy scores\")\n",
    "    \n",
    "    # Detect anomalies in transaction patterns\n",
    "    anomalies = []\n",
    "    \n",
    "    # Check amount distribution for suspicious uniformity\n",
    "    if \"amount\" in tx_df.columns:\n",
    "        amount_anomalies = detect_entropy_anomalies(tx_df[\"amount\"].dropna().values)\n",
    "        if amount_anomalies:\n",
    "            anomalies.extend(amount_anomalies)\n",
    "    \n",
    "    # Check time interval distribution for suspicious patterns\n",
    "    if \"time_diff\" in tx_df.columns:\n",
    "        time_anomalies = detect_entropy_anomalies(tx_df[\"time_diff\"].dropna().values)\n",
    "        if time_anomalies:\n",
    "            anomalies.extend(time_anomalies)\n",
    "    \n",
    "    print(f\"Detected {len(anomalies)} anomalies in transaction patterns\")\n",
    "    \n",
    "    # Visualize entropy distribution\n",
    "    if entropy_scores:\n",
    "        features = [score[\"feature\"] for score in entropy_scores]\n",
    "        entropies = [score[\"entropy\"] for score in entropy_scores]\n",
    "        normalized_entropies = [score[\"normalized_entropy\"] for score in entropy_scores]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(features, normalized_entropies, alpha=0.7)\n",
    "        plt.title(\"Normalized Entropy by Transaction Feature\")\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Normalized Entropy (0-1)\")\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        \"entropy_scores\": entropy_scores,\n",
    "        \"anomalies\": anomalies\n",
    "    }\n",
    "\n",
    "# Calculate entropy scores for mixer transactions\n",
    "entropy_analysis = calculate_mixer_entropy_scores(mixer_transaction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20a261",
   "metadata": {},
   "source": [
    "## Transaction Flow Graph Analysis\n",
    "\n",
    "Let's analyze the transaction flow graph to identify mixer-like patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0581e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No transactions available for flow graph analysis\n"
     ]
    }
   ],
   "source": [
    "# Create transaction flow graph from mixer transactions\n",
    "def create_mixer_flow_graph(tx_data: Dict) -> TransactionFlowGraph:\n",
    "    transactions = tx_data.get(\"transactions\", [])\n",
    "    if not transactions:\n",
    "        print(\"No transactions available for flow graph analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Creating transaction flow graph from {len(transactions)} transactions...\")\n",
    "    \n",
    "    # Initialize transaction flow graph\n",
    "    flow_graph = TransactionFlowGraph()\n",
    "    \n",
    "    # Add transactions to the graph\n",
    "    for tx in transactions:\n",
    "        # Extract source and target addresses\n",
    "        source_addr = tx.get(\"source_address\") or tx.get(\"sender_address\") or tx.get(\"from\")\n",
    "        target_addr = tx.get(\"target_address\") or tx.get(\"receiver_address\") or tx.get(\"to\")\n",
    "        \n",
    "        if source_addr and target_addr:\n",
    "            # Add edge to graph with transaction data\n",
    "            amount = tx.get(\"amount\", 0)\n",
    "            timestamp = tx.get(\"block_time\", 0)\n",
    "            tx_hash = tx.get(\"signature\") or tx.get(\"tx_hash\") or \"unknown\"\n",
    "            \n",
    "            flow_graph.add_transaction(\n",
    "                source_addr, \n",
    "                target_addr, \n",
    "                amount=amount,\n",
    "                timestamp=timestamp,\n",
    "                tx_hash=tx_hash,\n",
    "                mixer_name=tx.get(\"mixer_name\", \"\")\n",
    "            )\n",
    "    \n",
    "    # Add additional details to nodes\n",
    "    for address in flow_graph.get_nodes():\n",
    "        is_mixer, mixer_name = is_known_mixer(address)\n",
    "        if is_mixer:\n",
    "            flow_graph.set_node_attribute(address, \"is_mixer\", True)\n",
    "            flow_graph.set_node_attribute(address, \"mixer_name\", mixer_name)\n",
    "        else:\n",
    "            flow_graph.set_node_attribute(address, \"is_mixer\", False)\n",
    "    \n",
    "    # Calculate graph metrics\n",
    "    centrality = flow_graph.calculate_centrality()\n",
    "    communities = flow_graph.identify_communities()\n",
    "    \n",
    "    print(f\"Flow graph created with {len(flow_graph.get_nodes())} nodes and {flow_graph.get_edge_count()} edges\")\n",
    "    print(f\"Detected {len(communities)} communities in the transaction network\")\n",
    "    \n",
    "    return flow_graph\n",
    "\n",
    "# Create mixer flow graph\n",
    "mixer_flow_graph = create_mixer_flow_graph(mixer_transaction_data)\n",
    "\n",
    "# Visualize the transaction flow graph\n",
    "if mixer_flow_graph:\n",
    "    # Export graph data for visualization\n",
    "    graph_data = mixer_flow_graph.export_to_json()\n",
    "    \n",
    "    # Save graph data\n",
    "    os.makedirs(\"../../data/output\", exist_ok=True)\n",
    "    with open(\"../../data/output/mixer_flow_graph.json\", \"w\") as f:\n",
    "        json.dump(graph_data, f)\n",
    "    \n",
    "    # Visualize the graph using our utility\n",
    "    viz_file = visualize_transaction_flow(\n",
    "        mixer_flow_graph,\n",
    "        highlight_nodes=all_mixer_addresses,\n",
    "        output_file=\"../../data/visualizations/mixer_flow_graph.png\"\n",
    "    )\n",
    "    \n",
    "    # Display visualization\n",
    "    if viz_file and os.path.exists(viz_file):\n",
    "        from IPython.display import Image\n",
    "        display(Image(filename=viz_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de0c83",
   "metadata": {},
   "source": [
    "## Identify Mixer Users\n",
    "\n",
    "Now, let's identify addresses that have interacted with mixer services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f01b7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No flow graph available for mixer user analysis\n"
     ]
    }
   ],
   "source": [
    "# Function to identify mixer users from transaction flow graph\n",
    "def identify_mixer_users(flow_graph: TransactionFlowGraph) -> Dict:\n",
    "    if not flow_graph:\n",
    "        print(\"No flow graph available for mixer user analysis\")\n",
    "        return {\"depositors\": [], \"withdrawers\": [], \"suspicious_users\": []}\n",
    "    \n",
    "    print(\"Identifying addresses that have interacted with mixers...\")\n",
    "    \n",
    "    # Get all known mixer addresses in the graph\n",
    "    mixer_nodes = []\n",
    "    for node in flow_graph.get_nodes():\n",
    "        if flow_graph.get_node_attribute(node, \"is_mixer\"):\n",
    "            mixer_nodes.append(node)\n",
    "    \n",
    "    print(f\"Found {len(mixer_nodes)} mixer addresses in the graph\")\n",
    "    \n",
    "    # Identify depositors (addresses that sent funds to mixers)\n",
    "    depositors = {}\n",
    "    for mixer_addr in mixer_nodes:\n",
    "        in_neighbors = flow_graph.get_in_neighbors(mixer_addr)\n",
    "        for neighbor in in_neighbors:\n",
    "            # Get transaction details\n",
    "            edge_data = flow_graph.get_edge_attributes(neighbor, mixer_addr)\n",
    "            for tx in edge_data.get(\"transactions\", []):\n",
    "                # Create depositor entry if it doesn't exist\n",
    "                if neighbor not in depositors:\n",
    "                    depositors[neighbor] = {\n",
    "                        \"address\": neighbor,\n",
    "                        \"mixers_used\": {},\n",
    "                        \"total_deposit_amount\": 0,\n",
    "                        \"transaction_count\": 0\n",
    "                    }\n",
    "                \n",
    "                # Add mixer to this depositor's list if not already there\n",
    "                mixer_name = tx.get(\"mixer_name\", \"unknown_mixer\")\n",
    "                if mixer_name not in depositors[neighbor][\"mixers_used\"]:\n",
    "                    depositors[neighbor][\"mixers_used\"][mixer_name] = 0\n",
    "                \n",
    "                # Update statistics\n",
    "                amount = tx.get(\"amount\", 0)\n",
    "                depositors[neighbor][\"mixers_used\"][mixer_name] += amount\n",
    "                depositors[neighbor][\"total_deposit_amount\"] += amount\n",
    "                depositors[neighbor][\"transaction_count\"] += 1\n",
    "    \n",
    "    # Identify withdrawers (addresses that received funds from mixers)\n",
    "    withdrawers = {}\n",
    "    for mixer_addr in mixer_nodes:\n",
    "        out_neighbors = flow_graph.get_out_neighbors(mixer_addr)\n",
    "        for neighbor in out_neighbors:\n",
    "            # Get transaction details\n",
    "            edge_data = flow_graph.get_edge_attributes(mixer_addr, neighbor)\n",
    "            for tx in edge_data.get(\"transactions\", []):\n",
    "                # Create withdrawer entry if it doesn't exist\n",
    "                if neighbor not in withdrawers:\n",
    "                    withdrawers[neighbor] = {\n",
    "                        \"address\": neighbor,\n",
    "                        \"mixers_used\": {},\n",
    "                        \"total_withdrawal_amount\": 0,\n",
    "                        \"transaction_count\": 0\n",
    "                    }\n",
    "                \n",
    "                # Add mixer to this withdrawer's list if not already there\n",
    "                mixer_name = tx.get(\"mixer_name\", \"unknown_mixer\")\n",
    "                if mixer_name not in withdrawers[neighbor][\"mixers_used\"]:\n",
    "                    withdrawers[neighbor][\"mixers_used\"][mixer_name] = 0\n",
    "                \n",
    "                # Update statistics\n",
    "                amount = tx.get(\"amount\", 0)\n",
    "                withdrawers[neighbor][\"mixers_used\"][mixer_name] += amount\n",
    "                withdrawers[neighbor][\"total_withdrawal_amount\"] += amount\n",
    "                withdrawers[neighbor][\"transaction_count\"] += 1\n",
    "    \n",
    "    # Identify particularly suspicious users (involved in multiple mixers or with high volumes)\n",
    "    suspicious_users = []\n",
    "    \n",
    "    # Check depositors\n",
    "    for addr, data in depositors.items():\n",
    "        # Flag if using multiple mixers\n",
    "        if len(data[\"mixers_used\"]) > 1:\n",
    "            suspicious_users.append({\n",
    "                \"address\": addr,\n",
    "                \"reason\": f\"Deposited to {len(data['mixers_used'])} different mixers\",\n",
    "                \"risk_score\": min(85, 50 + 10 * len(data[\"mixers_used\"])),  # Higher for more mixers\n",
    "                \"role\": \"depositor\",\n",
    "                \"total_amount\": data[\"total_deposit_amount\"]\n",
    "            })\n",
    "        # Flag if high volume\n",
    "        elif data[\"total_deposit_amount\"] > 10000:  # $10k threshold\n",
    "            suspicious_users.append({\n",
    "                \"address\": addr,\n",
    "                \"reason\": f\"High-volume mixer deposits (${data['total_deposit_amount']:.2f})\",\n",
    "                \"risk_score\": min(90, 40 + int(data[\"total_deposit_amount\"] / 1000)),\n",
    "                \"role\": \"depositor\",\n",
    "                \"total_amount\": data[\"total_deposit_amount\"]\n",
    "            })\n",
    "    \n",
    "    # Check withdrawers\n",
    "    for addr, data in withdrawers.items():\n",
    "        # Flag if using multiple mixers\n",
    "        if len(data[\"mixers_used\"]) > 1:\n",
    "            suspicious_users.append({\n",
    "                \"address\": addr,\n",
    "                \"reason\": f\"Withdrew from {len(data['mixers_used'])} different mixers\",\n",
    "                \"risk_score\": min(85, 50 + 10 * len(data[\"mixers_used\"])),\n",
    "                \"role\": \"withdrawer\",\n",
    "                \"total_amount\": data[\"total_withdrawal_amount\"]\n",
    "            })\n",
    "        # Flag if high volume\n",
    "        elif data[\"total_withdrawal_amount\"] > 10000:  # $10k threshold\n",
    "            suspicious_users.append({\n",
    "                \"address\": addr,\n",
    "                \"reason\": f\"High-volume mixer withdrawals (${data['total_withdrawal_amount']:.2f})\",\n",
    "                \"risk_score\": min(90, 40 + int(data[\"total_withdrawal_amount\"] / 1000)),\n",
    "                \"role\": \"withdrawer\",\n",
    "                \"total_amount\": data[\"total_withdrawal_amount\"]\n",
    "            })\n",
    "    \n",
    "    # Sort suspicious users by risk score\n",
    "    suspicious_users.sort(key=lambda x: x[\"risk_score\"], reverse=True)\n",
    "    \n",
    "    print(f\"Identified {len(depositors)} depositors and {len(withdrawers)} withdrawers\")\n",
    "    print(f\"Flagged {len(suspicious_users)} particularly suspicious users\")\n",
    "    if suspicious_users:\n",
    "        print(\"\\nTop suspicious users:\")\n",
    "        for user in suspicious_users[:3]:  # Show top 3\n",
    "            print(f\"- {user['address']} | {user['reason']} | Risk: {user['risk_score']}\")\n",
    "    \n",
    "    return {\n",
    "        \"depositors\": list(depositors.values()),\n",
    "        \"withdrawers\": list(withdrawers.values()),\n",
    "        \"suspicious_users\": suspicious_users\n",
    "    }\n",
    "\n",
    "# Identify mixer users\n",
    "mixer_users = identify_mixer_users(mixer_flow_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c241d7",
   "metadata": {},
   "source": [
    "## Detect Mixer-Like Behavior in Unknown Addresses\n",
    "\n",
    "Let's look for addresses that behave like mixers even if they're not known mixers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "183fe325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No flow graph available for mixer-like behavior detection\n"
     ]
    }
   ],
   "source": [
    "# Function to detect mixer-like behavior in non-mixer addresses\n",
    "def detect_mixer_like_behavior(flow_graph: TransactionFlowGraph, known_mixers: List[str] = None) -> List[Dict]:\n",
    "    if not flow_graph:\n",
    "        print(\"No flow graph available for mixer-like behavior detection\")\n",
    "        return []\n",
    "    \n",
    "    if known_mixers is None:\n",
    "        known_mixers = all_mixer_addresses\n",
    "        \n",
    "    print(\"Detecting addresses with mixer-like behavior...\")\n",
    "    \n",
    "    # Candidate addresses (exclude known mixers)\n",
    "    candidates = [node for node in flow_graph.get_nodes() \n",
    "                 if node not in known_mixers and not flow_graph.get_node_attribute(node, \"is_mixer\")]\n",
    "    \n",
    "    # Initialize results\n",
    "    mixer_like_addresses = []\n",
    "    \n",
    "    # For each candidate, check if it exhibits mixer-like behavior\n",
    "    for candidate in candidates:\n",
    "        # Calculate metrics\n",
    "        in_degree = len(flow_graph.get_in_neighbors(candidate))\n",
    "        out_degree = len(flow_graph.get_out_neighbors(candidate))\n",
    "        \n",
    "        # Skip if not enough connections\n",
    "        if in_degree < 3 or out_degree < 3:\n",
    "            continue\n",
    "            \n",
    "        # Get all incoming and outgoing transactions\n",
    "        in_txs = []\n",
    "        for neighbor in flow_graph.get_in_neighbors(candidate):\n",
    "            edge_data = flow_graph.get_edge_attributes(neighbor, candidate)\n",
    "            in_txs.extend(edge_data.get(\"transactions\", []))\n",
    "            \n",
    "        out_txs = []\n",
    "        for neighbor in flow_graph.get_out_neighbors(candidate):\n",
    "            edge_data = flow_graph.get_edge_attributes(candidate, neighbor)\n",
    "            out_txs.extend(edge_data.get(\"transactions\", []))\n",
    "        \n",
    "        # Analyze transaction patterns\n",
    "        mixer_score = 0\n",
    "        reasons = []\n",
    "        \n",
    "        # Check for many small inputs and outputs (typical of mixers)\n",
    "        if in_degree > 10 and out_degree > 10:\n",
    "            mixer_score += 30\n",
    "            reasons.append(f\"High connectivity: {in_degree} inputs, {out_degree} outputs\")\n",
    "        \n",
    "        # Check for similar sized outputs (typical of mixers)\n",
    "        if out_txs:\n",
    "            out_amounts = [tx.get(\"amount\", 0) for tx in out_txs]\n",
    "            out_std = np.std(out_amounts)\n",
    "            out_mean = np.mean(out_amounts)\n",
    "            out_cv = out_std / out_mean if out_mean > 0 else 0\n",
    "            \n",
    "            if out_cv < 0.1 and len(out_txs) > 5:  # Very uniform outputs\n",
    "                mixer_score += 40\n",
    "                reasons.append(f\"Highly uniform output amounts (CV: {out_cv:.4f})\")\n",
    "            elif out_cv < 0.25 and len(out_txs) > 5:  # Somewhat uniform\n",
    "                mixer_score += 20\n",
    "                reasons.append(f\"Moderately uniform output amounts (CV: {out_cv:.4f})\")\n",
    "        \n",
    "        # Check for timing patterns (typical of mixers)\n",
    "        if out_txs:\n",
    "            timestamps = sorted([tx.get(\"timestamp\", 0) for tx in out_txs if tx.get(\"timestamp\", 0) > 0])\n",
    "            if len(timestamps) > 5:\n",
    "                time_diffs = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "                time_cv = np.std(time_diffs) / np.mean(time_diffs) if np.mean(time_diffs) > 0 else 0\n",
    "                \n",
    "                if time_cv < 0.2:  # Very regular timing\n",
    "                    mixer_score += 30\n",
    "                    reasons.append(f\"Regular transaction timing (CV: {time_cv:.4f})\")\n",
    "        \n",
    "        # Check for high turnover (inputs close to outputs)\n",
    "        in_total = sum(tx.get(\"amount\", 0) for tx in in_txs)\n",
    "        out_total = sum(tx.get(\"amount\", 0) for tx in out_txs)\n",
    "        \n",
    "        if in_total > 0 and out_total > 0:\n",
    "            turnover_ratio = out_total / in_total\n",
    "            if 0.9 < turnover_ratio < 1.1:  # Near-perfect balance\n",
    "                mixer_score += 20\n",
    "                reasons.append(f\"Balanced input/output ratio: {turnover_ratio:.2f}\")\n",
    "        \n",
    "        # If score is high enough, consider it mixer-like\n",
    "        if mixer_score >= 50:\n",
    "            risk_level = \"high\" if mixer_score >= 80 else \"medium\" if mixer_score >= 65 else \"low\"\n",
    "            \n",
    "            mixer_like_addresses.append({\n",
    "                \"address\": candidate,\n",
    "                \"mixer_score\": mixer_score,\n",
    "                \"risk_level\": risk_level,\n",
    "                \"in_degree\": in_degree,\n",
    "                \"out_degree\": out_degree,\n",
    "                \"in_amount\": in_total,\n",
    "                \"out_amount\": out_total,\n",
    "                \"reasons\": reasons\n",
    "            })\n",
    "    \n",
    "    # Sort by mixer score (highest first)\n",
    "    mixer_like_addresses.sort(key=lambda x: x[\"mixer_score\"], reverse=True)\n",
    "    \n",
    "    print(f\"Identified {len(mixer_like_addresses)} addresses with mixer-like behavior\")\n",
    "    if mixer_like_addresses:\n",
    "        print(\"\\nTop suspected mixers:\")\n",
    "        for addr in mixer_like_addresses[:3]:  # Show top 3\n",
    "            print(f\"- {addr['address']} | Score: {addr['mixer_score']} | Risk: {addr['risk_level']}\")\n",
    "            for reason in addr[\"reasons\"]:\n",
    "                print(f\"  * {reason}\")\n",
    "    \n",
    "    return mixer_like_addresses\n",
    "\n",
    "# Detect mixer-like behavior\n",
    "suspected_mixers = detect_mixer_like_behavior(mixer_flow_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59007c",
   "metadata": {},
   "source": [
    "## Generate Conclusions and Report\n",
    "\n",
    "Let's create a comprehensive report of our findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fc46f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report generated and saved to ../../reports/mixer_analysis_report_20250424_223407.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Cryptocurrency Mixer Analysis Report\n",
       "\n",
       "Generated on: 2025-04-24 22:34:07\n",
       "\n",
       "## Known Mixer Services\n",
       "\n",
       "### Tornado Cash Solana\n",
       "\n",
       "Risk level: **HIGH**\n",
       "\n",
       "**Addresses:**\n",
       "- `TCnifB7JjcmXP5F7hJ9wQDEq47Qympz4JbRRUBtcEJ8`\n",
       "- `TCxZde8bp2sp5s7fK8K4nnzJWXpMjHmYLAXPYvgdz4Z`\n",
       "\n",
       "**Typical Transaction Patterns:**\n",
       "- Equal Amounts\n",
       "- Fixed Intervals\n",
       "- Privacy Set\n",
       "\n",
       "### Elusiv\n",
       "\n",
       "Risk level: **MEDIUM**\n",
       "\n",
       "**Addresses:**\n",
       "- `E1w8SZpBPkRBdBmEJUwpRZx1SbQVVhNqkuH95uKJvypH`\n",
       "- `2EgZ5LuMqyVKQYS4AFhJRpZNr1rLxuU4UJuNJbcKFubu`\n",
       "\n",
       "**Typical Transaction Patterns:**\n",
       "- Stealth Addresses\n",
       "- Ring Signature\n",
       "- Decoy Outputs\n",
       "\n",
       "### Cyclos Mixer\n",
       "\n",
       "Risk level: **MEDIUM**\n",
       "\n",
       "**Addresses:**\n",
       "- `CYcLEsDHNZn8mVimVJLMFeYRGzdPx9QmxUL5kgKSTsdq`\n",
       "- `CYCSaAMM4tJLXeXQKVAZrkLUKu8gYXhRNfYJG5qkKPQt`\n",
       "\n",
       "**Typical Transaction Patterns:**\n",
       "- Pool Deposits\n",
       "- Time Locks\n",
       "- Uniform Withdrawals\n",
       "\n",
       "## Transaction Flow Visualization\n",
       "\n",
       "The following graph shows the flow of funds through mixer services:\n",
       "\n",
       "![Transaction Flow](../../data/visualizations/mixer_flow_graph.png)\n",
       "\n",
       "## Mixer Detection Recommendations\n",
       "\n",
       "1. **Monitor high-risk addresses** - Track transactions from addresses identified as high-risk mixer users\n",
       "2. **Implement transaction entropy analysis** - Deploy real-time entropy analysis to detect mixer-like patterns\n",
       "3. **Graph analytics** - Use graph algorithms to identify suspicious fund flows through mixers\n",
       "4. **Regular updates to mixer database** - Keep the list of known mixer addresses current\n",
       "5. **Temporal pattern analysis** - Look for regular time intervals in transaction patterns\n",
       "6. **Amount uniformity checks** - Flag transactions with suspiciously uniform amounts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate mixer analysis report\n",
    "def generate_mixer_report() -> str:\n",
    "    report = [\"# Cryptocurrency Mixer Analysis Report\\n\"]\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    \n",
    "    # Known mixers analysis\n",
    "    report.append(\"## Known Mixer Services\\n\")\n",
    "    for mixer_name, mixer_info in KNOWN_MIXERS.items():\n",
    "        report.append(f\"### {mixer_name.replace('_', ' ').title()}\\n\")\n",
    "        report.append(f\"Risk level: **{mixer_info['risk_level'].upper()}**\\n\")\n",
    "        report.append(f\"**Addresses:**\")\n",
    "        for addr in mixer_info[\"addresses\"]:\n",
    "            report.append(f\"- `{addr}`\")\n",
    "        \n",
    "        report.append(\"\\n**Typical Transaction Patterns:**\")\n",
    "        for pattern in mixer_info[\"transaction_patterns\"]:\n",
    "            report.append(f\"- {pattern.replace('_', ' ').title()}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Transaction patterns and entropy analysis\n",
    "    entropy_scores = entropy_analysis.get(\"entropy_scores\", [])\n",
    "    anomalies = entropy_analysis.get(\"anomalies\", [])\n",
    "    \n",
    "    if entropy_scores or anomalies:\n",
    "        report.append(\"## Transaction Pattern Analysis\\n\")\n",
    "        \n",
    "        if entropy_scores:\n",
    "            report.append(\"### Entropy Analysis\\n\")\n",
    "            report.append(\"Higher entropy indicates more randomness and potential obfuscation:\\n\")\n",
    "            \n",
    "            report.append(\"| Feature | Entropy | Normalized Entropy |\")\n",
    "            report.append(\"| ------- | ------- | ------------------ |\")\n",
    "            for score in entropy_scores:\n",
    "                report.append(f\"| {score['feature']} | {score['entropy']:.4f} | {score['normalized_entropy']:.4f} |\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        if anomalies:\n",
    "            report.append(\"### Detected Anomalies\\n\")\n",
    "            for anomaly in anomalies:\n",
    "                report.append(f\"- **{anomaly['type']}**: {anomaly['description']}\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Suspicious users analysis\n",
    "    suspicious_users = mixer_users.get(\"suspicious_users\", [])\n",
    "    if suspicious_users:\n",
    "        report.append(\"## Suspicious Address Analysis\\n\")\n",
    "        report.append(\"### High-Risk Mixer Users\\n\")\n",
    "        report.append(\"| Address | Role | Risk Score | Reason | Amount |\")\n",
    "        report.append(\"| ------- | ---- | ---------- | ------ | ------ |\")\n",
    "        for user in suspicious_users:\n",
    "            report.append(f\"| `{user['address']}` | {user['role'].title()} | {user['risk_score']} | {user['reason']} | ${user['total_amount']:.2f} |\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Suspected mixers analysis\n",
    "    if suspected_mixers:\n",
    "        report.append(\"## Suspected Mixer Services\\n\")\n",
    "        report.append(\"These addresses exhibit behavior consistent with cryptocurrency mixers:\\n\")\n",
    "        \n",
    "        report.append(\"| Address | Score | Risk Level | In/Out Connections | In/Out Volume |\")\n",
    "        report.append(\"| ------- | ----- | ---------- | ------------------ | ------------- |\")\n",
    "        for mixer in suspected_mixers:\n",
    "            report.append(f\"| `{mixer['address']}` | {mixer['mixer_score']} | {mixer['risk_level'].upper()} | {mixer['in_degree']}/{mixer['out_degree']} | ${mixer['in_amount']:.2f}/${mixer['out_amount']:.2f} |\")\n",
    "        \n",
    "        report.append(\"\\n### Detection Reasons\\n\")\n",
    "        for mixer in suspected_mixers[:5]:  # Show top 5\n",
    "            report.append(f\"**{mixer['address']}**:\")\n",
    "            for reason in mixer[\"reasons\"]:\n",
    "                report.append(f\"- {reason}\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Graph visualization\n",
    "    report.append(\"## Transaction Flow Visualization\\n\")\n",
    "    report.append(\"The following graph shows the flow of funds through mixer services:\\n\")\n",
    "    report.append(f\"![Transaction Flow](../../data/visualizations/mixer_flow_graph.png)\")\n",
    "    \n",
    "    # Mitigation recommendations\n",
    "    report.append(\"\\n## Mixer Detection Recommendations\\n\")\n",
    "    report.append(\"1. **Monitor high-risk addresses** - Track transactions from addresses identified as high-risk mixer users\")\n",
    "    report.append(\"2. **Implement transaction entropy analysis** - Deploy real-time entropy analysis to detect mixer-like patterns\")\n",
    "    report.append(\"3. **Graph analytics** - Use graph algorithms to identify suspicious fund flows through mixers\")\n",
    "    report.append(\"4. **Regular updates to mixer database** - Keep the list of known mixer addresses current\")\n",
    "    report.append(\"5. **Temporal pattern analysis** - Look for regular time intervals in transaction patterns\")\n",
    "    report.append(\"6. **Amount uniformity checks** - Flag transactions with suspiciously uniform amounts\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and save the report\n",
    "report_content = generate_mixer_report()\n",
    "\n",
    "# Save report to file\n",
    "os.makedirs(\"../../reports\", exist_ok=True)\n",
    "report_path = f\"../../reports/mixer_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"\\nReport generated and saved to {report_path}\")\n",
    "\n",
    "# Display report in notebook\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(report_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
